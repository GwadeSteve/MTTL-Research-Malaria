{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7793030",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing and Training Preparation\n",
    "This notebook implements preprocessing methods, parasitemia scoring, and data preparation for MTTL training\n",
    "\n",
    "## 2.1 Setup and Imports\n",
    "*We start being importing all necessary libraries and set up paths for preprocessing pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8efc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook')\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# If staintools not available, we'll fall back to our custom implementations\n",
    "try:\n",
    "    import staintools\n",
    "    STAINTOOLS_AVAILABLE = True\n",
    "    print(\"staintools library available\")\n",
    "except ImportError:\n",
    "    STAINTOOLS_AVAILABLE = False\n",
    "    print(\"staintools library not available - using alternative implementations\")\n",
    "    \n",
    "from src.utils.DataUtils import set_seeds, MalariaPreprocessor, MalariaDataPreparator\n",
    "set_seeds(12)\n",
    "\n",
    "# Target size\n",
    "TARGET_SIZE = (512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLM_ROOT = os.path.join('..', 'data', 'NIH-NLM-ThinBloodSmearsPf')\n",
    "POINT_SET_DIR = os.path.join(NLM_ROOT, 'Point Set')\n",
    "OUTPUT_DIR = os.path.join('..', 'data', 'preprocessed_NLM')\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'metadata'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb8e09",
   "metadata": {},
   "source": [
    "## 2.2 Data Loading and Path Setup\n",
    "*We then load the parsed annotations from Phase 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4190b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from Phase 1 \n",
    "def parse_point_set_annotations(point_set_dir):\n",
    "    annotations = []\n",
    "    point_set_folders = sorted([os.path.join(point_set_dir, d) for d in os.listdir(point_set_dir) \n",
    "                               if os.path.isdir(os.path.join(point_set_dir, d))])\n",
    "    \n",
    "    for folder in tqdm(point_set_folders, desc=\"Parsing annotations\"):\n",
    "        gt_dir = os.path.join(folder, 'GT')\n",
    "        img_dir = os.path.join(folder, 'Img')\n",
    "        if not os.path.isdir(gt_dir) or not os.path.isdir(img_dir):\n",
    "            continue\n",
    "            \n",
    "        for ann_file in os.listdir(gt_dir):\n",
    "            if not ann_file.lower().endswith('.txt'):\n",
    "                continue\n",
    "            ann_path = os.path.join(gt_dir, ann_file)\n",
    "            img_name = ann_file.replace('.txt', '.jpg')\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "                \n",
    "            with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.read().strip().split('\\n')\n",
    "                if len(lines) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                for line in lines[1:]:\n",
    "                    parts = line.split(',')\n",
    "                    if len(parts) < 7:\n",
    "                        continue\n",
    "                    cell_type = parts[1]\n",
    "                    shape = parts[3]\n",
    "                    \n",
    "                    if shape == 'Point':\n",
    "                        x = float(parts[5])\n",
    "                        y = float(parts[6])\n",
    "                        annotations.append({\n",
    "                            'image_path': img_path,\n",
    "                            'cell_type': cell_type,\n",
    "                            'shape': shape,\n",
    "                            'x': x,\n",
    "                            'y': y\n",
    "                        })\n",
    "                    elif shape == 'Polygon':\n",
    "                        n_points = int(parts[4])\n",
    "                        coords = [float(v) for v in parts[5:5+2*n_points]]\n",
    "                        xy = list(zip(coords[::2], coords[1::2]))\n",
    "                        xs, ys = zip(*xy)\n",
    "                        bbox = [min(xs), min(ys), max(xs), max(ys)]\n",
    "                        annotations.append({\n",
    "                            'image_path': img_path,\n",
    "                            'cell_type': cell_type,\n",
    "                            'shape': shape,\n",
    "                            'polygon': xy,\n",
    "                            'bbox': bbox\n",
    "                        })\n",
    "    return annotations\n",
    "\n",
    "print(\"Loading annotations...\")\n",
    "annotations = parse_point_set_annotations(POINT_SET_DIR)\n",
    "print(f\"Loaded {len(annotations)} annotations\")\n",
    "\n",
    "# Group annotations by image\n",
    "image_to_anns = defaultdict(list)\n",
    "for ann in annotations:\n",
    "    image_to_anns[ann['image_path']].append(ann)\n",
    "\n",
    "print(f\"Found {len(image_to_anns)} unique images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078f034",
   "metadata": {},
   "source": [
    "## 2.3 Test Image Selection\n",
    "*Next, We select representative test images for preprocessing method comparison*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selecting test images for preprocessing comparison...\")\n",
    "available_images = list(image_to_anns.keys())\n",
    "test_images = random.sample(available_images, min(3, len(available_images)))\n",
    "print(f\"Selected {len(test_images)} test images:\")\n",
    "for img in test_images:\n",
    "    print(f\"  - {os.path.basename(img)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d286a",
   "metadata": {},
   "source": [
    "## 2.4 Preprocessing Methods Implementation and Testing\n",
    "\n",
    "### 2.4.1 Preprocessing Methods Comparison\n",
    "*Test different preprocessing methods on sample images to determine the best approach*\n",
    "\n",
    "**Methods Tested:**\n",
    "- **Original**: Raw microscopy images\n",
    "- **Resized**: Standard resize to 224x224\n",
    "- **CLAHE**: Contrast Limited Adaptive Histogram Equalization \n",
    "- **Macenko**: Macenko stain normalization for consistent color\n",
    "- **Reinhard**: Reinhard color normalization\n",
    "- **Color Deconvolution**: Enhanced structure separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll compare preprocessing methods and their effects on original images\n",
    "def compare_preprocessing_methods(image_path, preprocessor):\n",
    "    # Load original image\n",
    "    original = np.array(Image.open(image_path).convert('RGB'))\n",
    "    \n",
    "    print(f\"\\nProcessing image: {os.path.basename(image_path)}\")\n",
    "    print(f\"Original image shape: {original.shape}, dtype: {original.dtype}\")\n",
    "    print(f\"Original intensity range: [{original.min()}, {original.max()}]\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Apply all methods\n",
    "    #resized = preprocessor.resize_image(original, maintain_aspect=False)\n",
    "    resized = preprocessor.resize_image(original, maintain_aspect=False)\n",
    "    clahe = preprocessor.clahe_normalization(resized.copy())\n",
    "    macenko = preprocessor.macenko_normalization(clahe.copy())\n",
    "    reinhard = preprocessor.reinhard_normalization(clahe.copy())\n",
    "    deconv = preprocessor.enhanced_color_deconvolution(clahe.copy())\n",
    "    \n",
    "    methods = {\n",
    "        'Original': original,\n",
    "        'Resized': resized,\n",
    "        'CLAHE': clahe,\n",
    "        'Macenko': macenko,\n",
    "        'Reinhard': reinhard,\n",
    "        'Color Deconv': deconv\n",
    "    }\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return methods\n",
    "\n",
    "# Test the preprocessing\n",
    "print(\"Testing preprocessing methods...\")\n",
    "preprocessor = MalariaPreprocessor(target_size=TARGET_SIZE)\n",
    "preprocessing_results = {}\n",
    "\n",
    "for img_path in test_images[:2]: \n",
    "    results = compare_preprocessing_methods(img_path, preprocessor)\n",
    "    preprocessing_results[os.path.basename(img_path)] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4df9b",
   "metadata": {},
   "source": [
    "### 2.4.2 Visual Preprocessing Comparison\n",
    "*Generate side-by-side visual comparisons of all preprocessing methods*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91492b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison plot of preprocessing methods\n",
    "def plot_preprocessing_comparison(results_dict):\n",
    "    \n",
    "    for img_name, results in results_dict.items():\n",
    "        # Order of methods to display\n",
    "        methods_order = ['Original', 'Resized', 'CLAHE', 'Macenko', 'Reinhard', 'Color Deconv']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle(f'Preprocessing Methods Comparison - {img_name}', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        for i, method in enumerate(methods_order):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            \n",
    "            if method in results:\n",
    "                axes[row, col].imshow(results[method])\n",
    "                axes[row, col].set_title(method, fontsize=16, fontweight='bold', pad=15)\n",
    "                axes[row, col].axis('off')\n",
    "                \n",
    "                # Subtle border around each image\n",
    "                for spine in axes[row, col].spines.values():\n",
    "                    spine.set_visible(True)\n",
    "                    spine.set_linewidth(2)\n",
    "                    spine.set_color('gray')\n",
    "        \n",
    "        # Spacing between subplots\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "        print(f\"Completed visualization for: {img_name}\\n\")\n",
    "\n",
    "# Visualization\n",
    "print(\"Visualizing preprocessing methods ...\")\n",
    "plot_preprocessing_comparison(preprocessing_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572eb7c",
   "metadata": {},
   "source": [
    "### 2.4.3 RGB Histogram Analysis\n",
    "*Analyze color distribution changes across different preprocessing methods*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB histograms for preprocessing comparison\n",
    "def plot_histogram_comparison(results_dict):\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    \n",
    "    for img_name, results in results_dict.items():\n",
    "        methods_order = ['Original', 'Resized', 'CLAHE', 'Macenko', 'Reinhard', 'Color Deconv']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle(f'RGB Histograms Comparison - {img_name}', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        for i, method in enumerate(methods_order):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            \n",
    "            if method in results:\n",
    "                img = results[method]\n",
    "                \n",
    "                # histogram for each RGB channel\n",
    "                for c, color in enumerate(colors):\n",
    "                    hist = cv2.calcHist([img], [c], None, [256], [0, 256])\n",
    "                    axes[row, col].plot(hist, color=color, alpha=0.8, linewidth=2, \n",
    "                                      label=f'{color.upper()} channel')\n",
    "                \n",
    "                axes[row, col].set_title(method, fontsize=14, fontweight='bold')\n",
    "                axes[row, col].set_xlabel('Pixel Intensity', fontsize=12)\n",
    "                axes[row, col].set_ylabel('Frequency', fontsize=12)\n",
    "                axes[row, col].legend(loc='upper right', fontsize=10)\n",
    "                axes[row, col].grid(True, alpha=0.3)\n",
    "                \n",
    "                # consistent y-axis limits\n",
    "                axes[row, col].set_ylim(0, None)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "        print(f\"Completed histogram analysis for: {img_name}\\n\")\n",
    "\n",
    "print(\"\\nGenerating histogram comparisons...\")\n",
    "plot_histogram_comparison(preprocessing_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041657fc",
   "metadata": {},
   "source": [
    "### 2.4.4 Quantitative Preprocessing Metrics\n",
    "*Calculate objective metrics to compare preprocessing effectiveness*\n",
    "\n",
    "**Metrics Calculated:**\n",
    "- **Contrast Ratio**: Standard deviation / mean intensity\n",
    "- **RMS Contrast**: Root mean square contrast measure\n",
    "- **Information Entropy**: Measures information content preservation\n",
    "- **Edge Strength**: Sobel gradient magnitude for cell boundary definition\n",
    "- **Dynamic Range**: Full intensity range utilization\n",
    "- **Signal-to-Noise Ratio**: Quality estimation using Laplacian variance\n",
    "- **Color Consistency**: Coefficient of variation across channels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quantitative metrics for preprocessing comparison\n",
    "def calculate_preprocessing_metrics(results_dict):\n",
    "    from scipy import stats\n",
    "    from skimage import measure, filters\n",
    "    \n",
    "    metrics_data = []\n",
    "    \n",
    "    print(\"Calculating preprocessing metrics...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for img_name, results in results_dict.items():\n",
    "        print(f\"\\nAnalyzing {img_name}:\")\n",
    "        \n",
    "        for method_name, img in results.items():\n",
    "            if method_name == 'Original':\n",
    "                continue\n",
    "            \n",
    "            # Convert to grayscale for some metrics\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # 1. Basic intensity statistics\n",
    "            mean_intensity = np.mean(img)\n",
    "            std_intensity = np.std(img)\n",
    "            \n",
    "            # 2. Contrast metrics\n",
    "            contrast_ratio = std_intensity / mean_intensity if mean_intensity > 0 else 0\n",
    "            rms_contrast = np.sqrt(np.mean((gray - np.mean(gray))**2))\n",
    "            \n",
    "            # 3. Entropy (information content)\n",
    "            hist, _ = np.histogram(gray, bins=256, range=(0,256))\n",
    "            hist = hist + 1e-10  # Avoid log(0)\n",
    "            hist_norm = hist / np.sum(hist)\n",
    "            entropy = -np.sum(hist_norm * np.log2(hist_norm))\n",
    "            \n",
    "            # 4. Color distribution metrics\n",
    "            color_std = [np.std(img[:,:,c]) for c in range(3)]\n",
    "            avg_color_std = np.mean(color_std)\n",
    "            \n",
    "            # 5. Edge content (Sobel gradient magnitude)\n",
    "            sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            edge_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "            avg_edge_strength = np.mean(edge_magnitude)\n",
    "            \n",
    "            # 6. Dynamic range\n",
    "            dynamic_range = np.max(img) - np.min(img)\n",
    "            \n",
    "            # 7. Signal-to-noise ratio estimation\n",
    "            # Using Laplacian variance as noise estimate\n",
    "            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            snr_estimate = np.mean(gray)**2 / (laplacian_var + 1e-10)\n",
    "            \n",
    "            # 8. Color consistency (coefficient of variation for each channel)\n",
    "            color_consistency = [std_intensity / (mean_intensity + 1e-10) for _ in range(3)]\n",
    "            avg_color_consistency = np.mean(color_consistency)\n",
    "            \n",
    "            metrics_data.append({\n",
    "                'Image': img_name,\n",
    "                'Method': method_name,\n",
    "                'Mean_Intensity': mean_intensity,\n",
    "                'Std_Intensity': std_intensity,\n",
    "                'Contrast_Ratio': contrast_ratio,\n",
    "                'RMS_Contrast': rms_contrast,\n",
    "                'Entropy': entropy,\n",
    "                'Avg_Color_Std': avg_color_std,\n",
    "                'Edge_Strength': avg_edge_strength,\n",
    "                'Dynamic_Range': dynamic_range,\n",
    "                'SNR_Estimate': snr_estimate,\n",
    "                'Color_Consistency': avg_color_consistency\n",
    "            })\n",
    "            \n",
    "            print(f\"  {method_name:12} | Contrast: {contrast_ratio:.3f} | Entropy: {entropy:.2f} | Edge: {avg_edge_strength:.1f}\")\n",
    "    \n",
    "    return pd.DataFrame(metrics_data)\n",
    "\n",
    "# Method selection and recommendation system\n",
    "def recommend_best_preprocessing_method(metrics_df, weights=None):\n",
    "    if weights is None:\n",
    "        # Default weights for our malaria microscopy based on what is important\n",
    "        weights = {\n",
    "            'contrast_weight': 0.25,      # Good contrast is important\n",
    "            'entropy_weight': 0.20,       # Information preservation\n",
    "            'edge_weight': 0.20,          # Cell boundary definition\n",
    "            'consistency_weight': 0.15,   # Color stability\n",
    "            'snr_weight': 0.10,          # Noise reduction\n",
    "            'dynamic_range_weight': 0.10  # Full intensity utilization\n",
    "        }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPROCESSING METHOD EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Summary statistics by method\n",
    "    method_stats = metrics_df.groupby('Method').agg({\n",
    "        'Contrast_Ratio': ['mean', 'std'],\n",
    "        'Entropy': ['mean', 'std'], \n",
    "        'Edge_Strength': ['mean', 'std'],\n",
    "        'Color_Consistency': ['mean', 'std'],\n",
    "        'SNR_Estimate': ['mean', 'std'],\n",
    "        'Dynamic_Range': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\nSUMMARY STATISTICS BY METHOD:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(method_stats)\n",
    "    \n",
    "    # Scoring system\n",
    "    methods = metrics_df['Method'].unique()\n",
    "    method_scores = {}\n",
    "    detailed_scores = {}\n",
    "    \n",
    "    print(f\"\\nDETAILED SCORING ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for method in methods:\n",
    "        method_data = metrics_df[metrics_df['Method'] == method]\n",
    "        \n",
    "        # Calculate normalized scores (0-1 scale)\n",
    "        avg_contrast = method_data['Contrast_Ratio'].mean()\n",
    "        avg_entropy = method_data['Entropy'].mean()\n",
    "        avg_edge = method_data['Edge_Strength'].mean()\n",
    "        avg_consistency = 1 / (method_data['Color_Consistency'].mean() + 1e-6)  # Lower is better\n",
    "        avg_snr = method_data['SNR_Estimate'].mean()\n",
    "        avg_dynamic_range = method_data['Dynamic_Range'].mean()\n",
    "        \n",
    "        # Normalize scores to 0-1 range based on optimal values for microscopy\n",
    "        contrast_score = min(avg_contrast / 0.4, 1.0)  # Optimal ~0.3-0.4\n",
    "        entropy_score = min(avg_entropy / 8.0, 1.0)    # Higher entropy is better\n",
    "        edge_score = min(avg_edge / 50.0, 1.0)         # Good edge definition\n",
    "        consistency_score = min(avg_consistency / 10.0, 1.0)  # Consistency important\n",
    "        snr_score = min(avg_snr / 100.0, 1.0)          # Good SNR\n",
    "        range_score = min(avg_dynamic_range / 255.0, 1.0)  # Full range utilization\n",
    "        \n",
    "        # weighted total score\n",
    "        total_score = (\n",
    "            contrast_score * weights['contrast_weight'] +\n",
    "            entropy_score * weights['entropy_weight'] +\n",
    "            edge_score * weights['edge_weight'] +\n",
    "            consistency_score * weights['consistency_weight'] +\n",
    "            snr_score * weights['snr_weight'] +\n",
    "            range_score * weights['dynamic_range_weight']\n",
    "        )\n",
    "        \n",
    "        method_scores[method] = total_score\n",
    "        detailed_scores[method] = {\n",
    "            'contrast': contrast_score,\n",
    "            'entropy': entropy_score,\n",
    "            'edge': edge_score,\n",
    "            'consistency': consistency_score,\n",
    "            'snr': snr_score,\n",
    "            'range': range_score,\n",
    "            'total': total_score\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        print(f\"Contrast Score:    {contrast_score:.3f} (avg ratio: {avg_contrast:.3f})\")\n",
    "        print(f\"Entropy Score:     {entropy_score:.3f} (avg entropy: {avg_entropy:.2f})\")\n",
    "        print(f\"Edge Score:        {edge_score:.3f} (avg strength: {avg_edge:.1f})\")\n",
    "        print(f\"Consistency Score: {consistency_score:.3f} (consistency: {1/avg_consistency:.3f})\")\n",
    "        print(f\"SNR Score:         {snr_score:.3f} (avg SNR: {avg_snr:.1f})\")\n",
    "        print(f\"Range Score:       {range_score:.3f} (avg range: {avg_dynamic_range:.0f})\")\n",
    "        print(f\"TOTAL SCORE:       {total_score:.3f}\")\n",
    "    \n",
    "    # Find best method\n",
    "    best_method_preprocessing = max(method_scores, key=method_scores.get)\n",
    "    best_score = method_scores[best_method_preprocessing]\n",
    "    \n",
    "    # Sort methods by score\n",
    "    ranked_methods = sorted(method_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RANKING:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, (method, score) in enumerate(ranked_methods, 1):\n",
    "        status = \"RECOMMENDED\" if method == best_method_preprocessing else f\"#{i}\"\n",
    "        print(f\"{status:15} {method:15} Score: {score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nBEST METHOD FOR MALARIA DETECTION: {best_method_preprocessing}\")\n",
    "    print(f\"   Final Score: {best_score:.3f}\")\n",
    "    \n",
    "    # Method-specific recommendations\n",
    "    print(f\"\\nANALYSIS SUMMARY:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if best_method_preprocessing == 'CLAHE':\n",
    "        print(\"CLAHE provides excellent contrast enhancement while preserving detail, Ideal for enhancing cell structures in microscopy images\")\n",
    "    elif best_method_preprocessing == 'Macenko':\n",
    "        print(\"Macenko normalization provides consistent stain appearance, Excellent for standardizing images across different preparations\")\n",
    "    elif best_method_preprocessing == 'Reinhard':\n",
    "        print(\"Reinhard normalization provides stable color distribution, Good for maintaining consistent color profiles\")\n",
    "    elif best_method_preprocessing == 'Color Deconv':\n",
    "        print(\"Color deconvolution enhances cellular structures, Excellent for separating different staining components\")\n",
    "    \n",
    "    return best_method_preprocessing, method_scores, detailed_scores\n",
    "\n",
    "# Visualization of metrics comparison\n",
    "def plot_metrics_comparison(metrics_df):\n",
    "    # plotting style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Preprocessing Methods - Quantitative Metrics Comparison', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    methods = metrics_df['Method'].unique()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(methods)))\n",
    "    \n",
    "    # 1. Contrast comparison\n",
    "    ax = axes[0, 0]\n",
    "    metrics_df.boxplot(column='Contrast_Ratio', by='Method', ax=ax)\n",
    "    ax.set_title('Contrast Ratio Distribution')\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Contrast Ratio')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 2. Entropy comparison\n",
    "    ax = axes[0, 1]\n",
    "    metrics_df.boxplot(column='Entropy', by='Method', ax=ax)\n",
    "    ax.set_title('Information Content (Entropy)')\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Entropy')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 3. Edge strength comparison\n",
    "    ax = axes[0, 2]\n",
    "    metrics_df.boxplot(column='Edge_Strength', by='Method', ax=ax)\n",
    "    ax.set_title('Edge Definition Strength')\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Edge Strength')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 4. SNR comparison\n",
    "    ax = axes[1, 0]\n",
    "    metrics_df.boxplot(column='SNR_Estimate', by='Method', ax=ax)\n",
    "    ax.set_title('Signal-to-Noise Ratio')\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('SNR Estimate')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 5. Dynamic range comparison\n",
    "    ax = axes[1, 1]\n",
    "    metrics_df.boxplot(column='Dynamic_Range', by='Method', ax=ax)\n",
    "    ax.set_title('Dynamic Range Utilization')\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Dynamic Range')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # 6. Overall score visualization\n",
    "    ax = axes[1, 2]\n",
    "    method_means = metrics_df.groupby('Method')[['Contrast_Ratio', 'Entropy', 'Edge_Strength']].mean()\n",
    "    method_means.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Key Metrics Overview')\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('Normalized Values')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61032b5d",
   "metadata": {},
   "source": [
    "### 2.4.5 Preprocessing Method Recommendation\n",
    "*Systematic evaluation and selection of optimal preprocessing method for malaria microscopy*\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- **Contrast Enhancement (25%)**: Good contrast crucial for cell differentiation\n",
    "- **Information Preservation (20%)**: Maintain diagnostic details\n",
    "- **Edge Definition (20%)**: Clear cell boundaries for detection\n",
    "- **Color Consistency (15%)**: Stable color profiles across images\n",
    "- **Noise Reduction (10%)**: Improved signal quality\n",
    "- **Dynamic Range (10%)**: Full intensity utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the analysis\n",
    "def run_complete_preprocessing_analysis():    \n",
    "    print(\"MALARIA PREPROCESSING ANALYSIS...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Calculate metrics\n",
    "    print(\"\\n1. Calculating metrics...\")\n",
    "    metrics_df = calculate_preprocessing_metrics(preprocessing_results)\n",
    "    \n",
    "    # 2. Display detailed metrics table\n",
    "    print(f\"\\n2. Detailed Metrics Table:\")\n",
    "    print(\"-\" * 60)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(metrics_df.round(3))\n",
    "    \n",
    "    # 3. Create metrics visualization\n",
    "    print(f\"\\n3. Generating metrics visualization...\")\n",
    "    plot_metrics_comparison(metrics_df)\n",
    "    \n",
    "    # 4. Recommend best method\n",
    "    print(f\"\\n4. Method recommendation analysis...\")\n",
    "    best_method_preprocessing, method_scores, detailed_scores = recommend_best_preprocessing_method(metrics_df)\n",
    "    \n",
    "    # 5. Save results\n",
    "    results_summary = {\n",
    "        'best_method_preprocessing': best_method_preprocessing,\n",
    "        'method_scores': method_scores,\n",
    "        'detailed_scores': detailed_scores,\n",
    "        'metrics_table': metrics_df.to_dict('records')\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    results_path = os.path.join(OUTPUT_DIR, 'preprocessing_analysis_results.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "    print(f\"\\nPREPROCESSING ANALYSIS COMPLETE!\")\n",
    "    print(f\"Recommended method: {best_method_preprocessing}\")\n",
    "    print(f\"Analysis based on {len(metrics_df)} measurements\")\n",
    "    \n",
    "    return best_method_preprocessing, metrics_df, method_scores\n",
    "\n",
    "# Run the analysis\n",
    "if 'preprocessing_results' in locals() and preprocessing_results:\n",
    "    best_method_preprocessing, metrics_df, scores = run_complete_preprocessing_analysis()\n",
    "else:\n",
    "    print(\"Error: preprocessing_results not found. Run the preprocessing comparison first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fb700",
   "metadata": {},
   "source": [
    "## 2.5 Parasitemia Scoring Implementation\n",
    "\n",
    "### 2.5.1 ParasitemiaScorer Initialization\n",
    "*Initialize the parasitemia scoring system with target image size*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1358b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.DataUtils import ParasitemiaScorer\n",
    "\n",
    "# Initialize parasitemia scorer\n",
    "print(\"\\nInitializing ParasitemiaScorer...\")\n",
    "parasitemia_scorer = ParasitemiaScorer(target_image_size=TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8f53f",
   "metadata": {},
   "source": [
    "### 2.5.2 Parasitemia Scoring Methods Testing\n",
    "*Test all three parasitemia scoring approaches on sample images*\n",
    "\n",
    "**Three Scoring Approaches Implemented:**\n",
    "\n",
    "1. **Count-based Method (Clinical Standard)**:\n",
    "   ```\n",
    "   Parasitemia = (Infected_Cells / Total_RBCs) × 100%\n",
    "   ```\n",
    "   - Most clinically relevant approach\n",
    "   - Direct percentage of infected red blood cells\n",
    "   - Standard medical practice metric from WHO\n",
    "\n",
    "2. **Area-based Method (Severity Weighting)**:\n",
    "   ```\n",
    "   Parasitemia = (Infected_Area / Total_Cell_Area) × 100%\n",
    "   ```\n",
    "   - Accounts for infection severity (larger parasites = higher score)\n",
    "   - Good for detecting advanced infections\n",
    "   - Considers parasite size variations\n",
    "\n",
    "3. **Density-based Method (Spatial Distribution)**:\n",
    "   ```\n",
    "   Parasitemia = (Infected_Objects / Image_Area) × 1000 per 1000px²\n",
    "   ```\n",
    "   - Normalizes by image area for consistency\n",
    "   - Spatial distribution aware\n",
    "   - Good for different cell density images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Test parasitemia scoring on sample data \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING PARASITEMIA SCORING ON SAMPLE DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def test_parasitemia_scoring_on_samples():\n",
    "    \n",
    "    print(f\"Testing on {len(test_images)} sample images...\")\n",
    "    \n",
    "    all_parasitemia_results = []\n",
    "    \n",
    "    for i, img_path in enumerate(test_images[:3], 1):  # Test on up to 3 images\n",
    "        print(f\"\\nProcessing Image {i}: {os.path.basename(img_path)}\")\n",
    "        \n",
    "        # Get annotations \n",
    "        if img_path in image_to_anns:\n",
    "            annotations = image_to_anns[img_path]\n",
    "            print(f\"Found {len(annotations)} annotations\")\n",
    "            \n",
    "            # Annotation types\n",
    "            cell_types = [ann.get('cell_type', 'unknown') for ann in annotations]\n",
    "            type_counts = Counter(cell_types)\n",
    "            print(f\"Cell types found: {dict(type_counts)}\")\n",
    "            \n",
    "            # Calculate parasitemia scores\n",
    "            results = parasitemia_scorer.calculate_all_scores(annotations, img_path)\n",
    "            all_parasitemia_results.append(results)\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\nPARASITEMIA SCORES:\")\n",
    "            print(f\"Count-based:   {results['count_based']['score']:.2f}%\")\n",
    "            print(f\"Area-based:    {results['area_based']['score']:.2f}%\")\n",
    "            print(f\"Density-based: {results['density_based']['score']:.2f} per 1000px²\")\n",
    "            \n",
    "            print(f\"\\nDETAILED BREAKDOWN:\")\n",
    "            print(f\"Infected cells: {results['summary']['infected_cells']}\")\n",
    "            print(f\"Healthy cells:  {results['summary']['healthy_cells']}\")\n",
    "            print(f\"Unknown cells:  {results['summary']['unknown_cells']}\")\n",
    "            print(f\"Total annotations: {results['summary']['total_annotations']}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"No annotations found for {img_path}\")\n",
    "    \n",
    "    return all_parasitemia_results\n",
    "\n",
    "# Running the test\n",
    "sample_results = test_parasitemia_scoring_on_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904b263",
   "metadata": {},
   "source": [
    "### 2.5.3 Comprehensive Parasitemia Analysis\n",
    "*Calculate parasitemia scores for all images using all three methods*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Calculate parasitemia for ALL images\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CALCULATING PARASITEMIA FOR ALL IMAGES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def calculate_parasitemia_for_all_images():\n",
    "    \"\"\"Calculate parasitemia scores for all available images\"\"\"\n",
    "    \n",
    "    print(f\"Processing {len(image_to_anns)} total images...\")\n",
    "    \n",
    "    all_results = []\n",
    "    failed_images = []\n",
    "    \n",
    "    for i, (img_path, annotations) in enumerate(tqdm(image_to_anns.items(), \n",
    "                                                   desc=\"Calculating parasitemia\")):\n",
    "        try:\n",
    "            results = parasitemia_scorer.calculate_all_scores(annotations, img_path)\n",
    "            all_results.append(results)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {os.path.basename(img_path)}: {str(e)}\")\n",
    "            failed_images.append((img_path, str(e)))\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(all_results)} images\")\n",
    "    if failed_images:\n",
    "        print(f\"Failed to process {len(failed_images)} images\")\n",
    "        for img_path, error in failed_images[:5]: \n",
    "            print(f\"  - {os.path.basename(img_path)}: {error}\")\n",
    "    \n",
    "    return all_results, failed_images\n",
    "\n",
    "# Calculate for all images\n",
    "print(\"Starting comprehensive parasitemia calculation...\")\n",
    "all_parasitemia_results, failed_images = calculate_parasitemia_for_all_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e471a0",
   "metadata": {},
   "source": [
    "### 2.5.4 Parasitemia Methods Comparison and Visualization\n",
    "*Generate comprehensive analysis plots comparing all scoring methods*\n",
    "\n",
    "**Analysis Includes:**\n",
    "- Score distribution histograms\n",
    "- Box plot comparisons\n",
    "- Correlation analysis between methods\n",
    "- Statistical summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbeeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Analyze and compare all scoring methods \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SCORING METHODS ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def create_parasitemia_analysis_plots(analysis_results):\n",
    "    if not analysis_results:\n",
    "        print(\"No analysis results to plot!\")\n",
    "        return\n",
    "    \n",
    "    stats = analysis_results['statistics']\n",
    "    all_results = analysis_results['all_results']\n",
    "    \n",
    "    # Data for plotting\n",
    "    count_scores = [r['count_based']['score'] for r in all_results]\n",
    "    area_scores = [r['area_based']['score'] for r in all_results]\n",
    "    density_scores = [r['density_based']['score'] for r in all_results]\n",
    "    \n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Parasitemia Scoring Methods - Comprehensive Analysis', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Score distributions\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(count_scores, bins=20, alpha=0.7, label='Count-based', color='blue')\n",
    "    ax.hist(area_scores, bins=20, alpha=0.7, label='Area-based', color='red')\n",
    "    ax.hist(density_scores, bins=20, alpha=0.7, label='Density-based', color='green')\n",
    "    ax.set_title('Score Distributions')\n",
    "    ax.set_xlabel('Parasitemia Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plots comparison\n",
    "    ax = axes[0, 1]\n",
    "    data_to_plot = [count_scores, area_scores, density_scores]\n",
    "    labels = ['Count-based', 'Area-based', 'Density-based']\n",
    "    box_plot = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "    colors = ['lightblue', 'lightcoral', 'lightgreen']\n",
    "    for patch, color in zip(box_plot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    ax.set_title('Score Distributions Comparison')\n",
    "    ax.set_ylabel('Parasitemia Score')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Count vs Area correlation\n",
    "    ax = axes[0, 2]\n",
    "    ax.scatter(count_scores, area_scores, alpha=0.6, color='purple')\n",
    "    ax.set_title('Count-based vs Area-based Scores')\n",
    "    ax.set_xlabel('Count-based Score (%)')\n",
    "    ax.set_ylabel('Area-based Score (%)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # correlation coefficient\n",
    "    corr = analysis_results['correlations']['count_vs_area']['correlation']\n",
    "    ax.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 4. Count vs Density correlation\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(count_scores, density_scores, alpha=0.6, color='orange')\n",
    "    ax.set_title('Count-based vs Density-based Scores')\n",
    "    ax.set_xlabel('Count-based Score (%)')\n",
    "    ax.set_ylabel('Density-based Score (per 1000px²)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    corr = analysis_results['correlations']['count_vs_density']['correlation']\n",
    "    ax.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 5. Area vs Density correlation\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(area_scores, density_scores, alpha=0.6, color='brown')\n",
    "    ax.set_title('Area-based vs Density-based Scores')\n",
    "    ax.set_xlabel('Area-based Score (%)')\n",
    "    ax.set_ylabel('Density-based Score (per 1000px²)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    corr = analysis_results['correlations']['area_vs_density']['correlation']\n",
    "    ax.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 6. Summary stats table\n",
    "    ax = axes[1, 2]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table_data = []\n",
    "    for method, stat in stats.items():\n",
    "        table_data.append([\n",
    "            method,\n",
    "            f\"{stat['mean']:.2f}\",\n",
    "            f\"{stat['std']:.2f}\",\n",
    "            f\"{stat['min']:.2f}\",\n",
    "            f\"{stat['max']:.2f}\"\n",
    "        ])\n",
    "    \n",
    "    table = ax.table(cellText=table_data,\n",
    "                    colLabels=['Method', 'Mean', 'Std', 'Min', 'Max'],\n",
    "                    cellLoc='center',\n",
    "                    loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    ax.set_title('Summary Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run analysis\n",
    "print(\"Analysis of all scoring methods...\")\n",
    "analysis_results = parasitemia_scorer.analyze_scoring_methods(all_parasitemia_results)\n",
    "\n",
    "if analysis_results:\n",
    "    print(\"\\nGenerating analysis plots...\")\n",
    "    create_parasitemia_analysis_plots(analysis_results)\n",
    "else:\n",
    "    print(\"Analysis failed - no results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df3e71",
   "metadata": {},
   "source": [
    "### 2.5.5 Parasitemia Method Selection\n",
    "*Systematic evaluation and recommendation of best parasitemia scoring method*\n",
    "\n",
    "**Selection Criteria:**\n",
    "- **Dynamic Range (25%)**: Ability to discriminate infection levels\n",
    "- **Coefficient of Variation (25%)**: Optimal balance (not too uniform/variable)\n",
    "- **Detection Rate (25%)**: Ability to detect parasitemia presence\n",
    "- **Clinical Relevance (25%)**: Medical standard alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Choose the best method and create final dataset \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD SELECTION AND FINAL DATASET CREATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def recommend_best_parasitemia_method(analysis_results):\n",
    "    \n",
    "    if not analysis_results:\n",
    "        print(\"No analysis results available!\")\n",
    "        return None\n",
    "    \n",
    "    stats = analysis_results['statistics']\n",
    "    correlations = analysis_results['correlations']\n",
    "    \n",
    "    print(\"METHOD RECOMMENDATION ANALYSIS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Scoring criteria for method selection\n",
    "    method_scores = {}\n",
    "    \n",
    "    for method, stat in stats.items():\n",
    "        print(f\"\\n{method.upper()} EVALUATION:\")\n",
    "        \n",
    "        # 1. Dynamic Range (higher is better for discrimination)\n",
    "        range_score = (stat['max'] - stat['min']) / stat['max'] if stat['max'] > 0 else 0\n",
    "        print(f\"Dynamic Range: {range_score:.3f} (range: {stat['min']:.2f}-{stat['max']:.2f})\")\n",
    "        \n",
    "        # 2. Coefficient of Variation (moderate is best, not too uniform, not too variable)\n",
    "        cv = stat['std'] / stat['mean'] if stat['mean'] > 0 else 0\n",
    "        cv_score = max(0, 1 - abs(cv - 0.5))  # Optimal CV around 0.5\n",
    "        print(f\"Coefficient of Variation: {cv:.3f} (score: {cv_score:.3f})\")\n",
    "        \n",
    "        # 3. Non-zero values (methods that can detect parasitemia)\n",
    "        non_zero_count = sum(1 for r in analysis_results['all_results'] \n",
    "                           if r[method.lower().replace('-', '_')]['score'] > 0)\n",
    "        non_zero_ratio = non_zero_count / len(analysis_results['all_results'])\n",
    "        print(f\"Detection Rate: {non_zero_ratio:.3f} ({non_zero_count}/{len(analysis_results['all_results'])})\")\n",
    "        \n",
    "        # 4. Clinical relevance\n",
    "        clinical_score = 1.0 if 'count' in method.lower() else 0.8 if 'area' in method.lower() else 0.6\n",
    "        print(f\"Clinical Relevance: {clinical_score:.3f}\")\n",
    "        \n",
    "        # 5. Calculate total score\n",
    "        total_score = (range_score * 0.25 + cv_score * 0.25 + \n",
    "                      non_zero_ratio * 0.25 + clinical_score * 0.25)\n",
    "        \n",
    "        method_scores[method] = {\n",
    "            'total_score': total_score,\n",
    "            'range_score': range_score,\n",
    "            'cv_score': cv_score,\n",
    "            'detection_rate': non_zero_ratio,\n",
    "            'clinical_score': clinical_score\n",
    "        }\n",
    "        \n",
    "        print(f\"TOTAL SCORE: {total_score:.3f}\")\n",
    "    \n",
    "    # Find best method\n",
    "    best_method_parasitemia = max(method_scores, key=lambda x: method_scores[x]['total_score'])\n",
    "    best_score = method_scores[best_method_parasitemia]['total_score']\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"FINAL RECOMMENDATION:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"BEST METHOD: {best_method_parasitemia}\")\n",
    "    print(f\"SCORE: {best_score:.3f}\")\n",
    "    \n",
    "    # Methods interpretation\n",
    "    if 'count' in best_method_parasitemia.lower():\n",
    "        print(\"\\nREASONING:\")\n",
    "        print(\"Count-based method is most clinically relevant, Directly represents percentage of infected cells, Standard approach in medical practice, Easy to interpret and validate\")\n",
    "    elif 'area' in best_method_parasitemia.lower():\n",
    "        print(\"\\nREASONING:\")\n",
    "        print(\"Area-based method accounts for infection severity, Larger parasites contribute more to score, Good for detecting advanced infections\")\n",
    "    else:\n",
    "        print(\"\\nREASONING:\")\n",
    "        print(\"Density-based method normalizes by image area, Good for comparing images with different cell densities, Spatial distribution aware\")\n",
    "    \n",
    "    return best_method_parasitemia, method_scores\n",
    "\n",
    "# Get recommendation\n",
    "best_method_parasitemia, method_scores = recommend_best_parasitemia_method(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_method_preprocessing = 'Color Deconv'\n",
    "#best_method_preprocessing = 'CLAHE'\n",
    "#best_method_preprocessing = 'Resized'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65452853",
   "metadata": {},
   "source": [
    "## 2.6 Final Dataset Creation Pipeline\n",
    "\n",
    "### 2.6.1 Final Dataset Creation with Chosen Methods\n",
    "*Create the complete dataset using the selected preprocessing and scoring methods*\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Image Processing**: Apply chosen preprocessing method\n",
    "2. **Annotation Resizing**: Scale annotations to target size (224×224)\n",
    "3. **Parasitemia Calculation**: Apply chosen scoring method\n",
    "4. **Infection Level Categorization**:\n",
    "   - **Negative**: 0% parasitemia\n",
    "   - **Low**: 0-2% parasitemia  \n",
    "   - **Moderate**: 2-10% parasitemia\n",
    "   - **High**: >10% parasitemia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: We will create final dataset with chosen method \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING FINAL DATASET WITH CHOSEN METHOD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_final_dataset(best_method, all_results, best_preprocessing_method):\n",
    "    \n",
    "    print(f\"Using {best_method} parasitemia scoring method\")\n",
    "    print(f\"Using {best_preprocessing_method} preprocessing method\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = MalariaPreprocessor(target_size=TARGET_SIZE)\n",
    "    \n",
    "    # Dataset structure\n",
    "    dataset_records = []\n",
    "    processed_images_count = 0\n",
    "    failed_processing = []\n",
    "    \n",
    "    # Create dir for resized annotations\n",
    "    resized_annotations_dir = os.path.join(OUTPUT_DIR, 'resized_annotations')\n",
    "    os.makedirs(resized_annotations_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nProcessing {len(all_results)} images for final dataset...\")\n",
    "    \n",
    "    for i, result in enumerate(tqdm(all_results, desc=\"Creating dataset\")):\n",
    "        try:\n",
    "            img_path = result['image_path']\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            original_img = np.array(Image.open(img_path).convert('RGB'))\n",
    "            original_size = original_img.shape\n",
    "            \n",
    "            # Apply preprocessing method\n",
    "            resized_img = preprocessor.resize_image(original_img, maintain_aspect=False)\n",
    "            \n",
    "            if best_preprocessing_method == 'CLAHE':\n",
    "                processed_img = preprocessor.clahe_normalization(resized_img.copy())\n",
    "            elif best_preprocessing_method == 'Macenko':\n",
    "                clahe_img = preprocessor.clahe_normalization(resized_img.copy())\n",
    "                processed_img = preprocessor.macenko_normalization(clahe_img.copy())\n",
    "            elif best_preprocessing_method == 'Reinhard':\n",
    "                clahe_img = preprocessor.clahe_normalization(resized_img.copy())\n",
    "                processed_img = preprocessor.reinhard_normalization(clahe_img.copy())\n",
    "            elif best_preprocessing_method == 'Color Deconv':\n",
    "                clahe_img = preprocessor.clahe_normalization(resized_img.copy())\n",
    "                processed_img = preprocessor.enhanced_color_deconvolution(clahe_img.copy())\n",
    "            else:\n",
    "                processed_img = resized_img  \n",
    "            \n",
    "            # Resize and save annotations \n",
    "            resized_annotations = []\n",
    "            resized_annotations_path = None\n",
    "            \n",
    "            if img_path in image_to_anns:\n",
    "                original_annotations = image_to_anns[img_path]\n",
    "                resized_annotations = preprocessor.resize_annotations_to_target_size(\n",
    "                    original_annotations, original_size, TARGET_SIZE\n",
    "                )\n",
    "                \n",
    "                # Save as JSON\n",
    "                ann_filename = f\"nlm_{i:04d}_resized_annotations.json\"\n",
    "                resized_annotations_path = os.path.join(resized_annotations_dir, ann_filename)\n",
    "                \n",
    "                with open(resized_annotations_path, 'w') as f:\n",
    "                    json.dump(resized_annotations, f, indent=2, default=str)\n",
    "            \n",
    "            # Extract parasitemia score based on chosen method\n",
    "            method_key = best_method.lower().replace('-', '_')\n",
    "            parasitemia_score = result[method_key]['score']\n",
    "            \n",
    "            # Determine infection level categories\n",
    "            if parasitemia_score == 0:\n",
    "                infection_level = 'negative'\n",
    "                infection_category = 0\n",
    "            elif parasitemia_score <= 2:\n",
    "                infection_level = 'low'\n",
    "                infection_category = 1\n",
    "            elif parasitemia_score <= 10:\n",
    "                infection_level = 'moderate'\n",
    "                infection_category = 2\n",
    "            else:\n",
    "                infection_level = 'high'\n",
    "                infection_category = 3\n",
    "            \n",
    "            # Create record\n",
    "            record = {\n",
    "                'image_id': f\"nlm_{i:04d}\",\n",
    "                'original_path': img_path,\n",
    "                'image_name': os.path.basename(img_path),\n",
    "                'processed_image': processed_img,\n",
    "                'parasitemia_score': parasitemia_score,\n",
    "                'infection_level': infection_level,\n",
    "                'infection_category': infection_category,\n",
    "                'cell_counts': {\n",
    "                    'infected': result['summary']['infected_cells'],\n",
    "                    'healthy': result['summary']['healthy_cells'],\n",
    "                    'wbc': result['summary']['wbc_cells'],\n",
    "                    'total_annotations': result['summary']['total_annotations']\n",
    "                },\n",
    "                'preprocessing_method': best_preprocessing_method,\n",
    "                'scoring_method': best_method,\n",
    "                'all_scores': {\n",
    "                    'count_based': result['count_based']['score'],\n",
    "                    'area_based': result['area_based']['score'],\n",
    "                    'density_based': result['density_based']['score']\n",
    "                },\n",
    "                'resized_annotations': resized_annotations,\n",
    "                'resized_annotations_path': resized_annotations_path\n",
    "            }\n",
    "            \n",
    "            dataset_records.append(record)\n",
    "            processed_images_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_processing.append((img_path, str(e)))\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nDataset creation complete!\")\n",
    "    print(f\"Successfully processed: {processed_images_count} images\")\n",
    "    print(f\"Failed to process: {len(failed_processing)} images\")\n",
    "    print(f\"Resized annotations saved to: {resized_annotations_dir}\")\n",
    "    \n",
    "    if failed_processing:\n",
    "        print(\"\\nFirst 3 processing failures:\")\n",
    "        for img_path, error in failed_processing[:3]:\n",
    "            print(f\"  - {os.path.basename(img_path)}: {error}\")\n",
    "    \n",
    "    return dataset_records\n",
    "\n",
    "\n",
    "def create_resized_annotation_lookup(final_dataset):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CREATING RESIZED ANNOTATION LOOKUP\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    resized_image_to_anns = {}\n",
    "    \n",
    "    for record in final_dataset:\n",
    "        if record.get('resized_annotations') and len(record['resized_annotations']) > 0:\n",
    "            # Use processed image path or image_id as key\n",
    "            processed_img_key = record['image_id']\n",
    "            resized_image_to_anns[processed_img_key] = record['resized_annotations']\n",
    "    \n",
    "    print(f\"Created resized annotation lookup for {len(resized_image_to_anns)} images\")\n",
    "    \n",
    "    # Lookup for future use\n",
    "    lookup_path = os.path.join(OUTPUT_DIR, 'resized_image_to_annotations_lookup.json')\n",
    "    with open(lookup_path, 'w') as f:\n",
    "        json.dump(resized_image_to_anns, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Saved resized annotation lookup: {lookup_path}\")\n",
    "    \n",
    "    return resized_image_to_anns\n",
    "\n",
    "\n",
    "def analyze_final_dataset(dataset_records):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL DATASET ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Extract data \n",
    "    parasitemia_scores = [r['parasitemia_score'] for r in dataset_records]\n",
    "    infection_levels = [r['infection_level'] for r in dataset_records]\n",
    "    infection_categories = [r['infection_category'] for r in dataset_records]\n",
    "    \n",
    "    # Distribution analysis\n",
    "    level_counts = Counter(infection_levels)\n",
    "    category_counts = Counter(infection_categories)\n",
    "    \n",
    "    print(f\"DATASET SIZE: {len(dataset_records)} images\")\n",
    "    print(f\"\\nINFECTION LEVEL DISTRIBUTION:\")\n",
    "    for level, count in level_counts.items():\n",
    "        percentage = (count / len(dataset_records)) * 100\n",
    "        print(f\"  {level:10}: {count:4d} images ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nPARASITEMIA SCORE STATISTICS:\")\n",
    "    print(f\"  Mean:     {np.mean(parasitemia_scores):.3f}\")\n",
    "    print(f\"  Std:      {np.std(parasitemia_scores):.3f}\")\n",
    "    print(f\"  Min:      {np.min(parasitemia_scores):.3f}\")\n",
    "    print(f\"  Max:      {np.max(parasitemia_scores):.3f}\")\n",
    "    print(f\"  Median:   {np.median(parasitemia_scores):.3f}\")\n",
    "    \n",
    "    print(f\"\\nCELL COUNT STATISTICS:\")\n",
    "    total_infected = sum(r['cell_counts']['infected'] for r in dataset_records)\n",
    "    total_healthy = sum(r['cell_counts']['healthy'] for r in dataset_records)\n",
    "    total_wbc = sum(r['cell_counts']['wbc'] for r in dataset_records)\n",
    "    total_annotations = sum(r['cell_counts']['total_annotations'] for r in dataset_records)\n",
    "    \n",
    "    print(f\"  Total infected cells:  {total_infected:,}\")\n",
    "    print(f\"  Total healthy cells:   {total_healthy:,}\")\n",
    "    print(f\"  Total WBC cells:       {total_wbc:,}\")\n",
    "    print(f\"  Total annotations:     {total_annotations:,}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Final Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Infection level distribution\n",
    "    ax = axes[0, 0]\n",
    "    levels = list(level_counts.keys())\n",
    "    counts = list(level_counts.values())\n",
    "    colors = ['green', 'yellow', 'orange', 'red'][:len(levels)]\n",
    "    ax.pie(counts, labels=levels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax.set_title('Infection Level Distribution')\n",
    "    \n",
    "    # 2. Parasitemia score histogram\n",
    "    ax = axes[0, 1]\n",
    "    ax.hist(parasitemia_scores, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax.set_title('Parasitemia Score Distribution')\n",
    "    ax.set_xlabel('Parasitemia Score (%)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Score vs infection level boxplot\n",
    "    ax = axes[1, 0]\n",
    "    level_data = []\n",
    "    level_labels = []\n",
    "    for level in ['negative', 'low', 'moderate', 'high']:\n",
    "        if level in level_counts:\n",
    "            scores = [r['parasitemia_score'] for r in dataset_records \n",
    "                     if r['infection_level'] == level]\n",
    "            if scores:\n",
    "                level_data.append(scores)\n",
    "                level_labels.append(level)\n",
    "    \n",
    "    box_plot = ax.boxplot(level_data, labels=level_labels, patch_artist=True)\n",
    "    colors = ['lightgreen', 'lightyellow', 'orange', 'lightcoral']\n",
    "    for patch, color in zip(box_plot['boxes'], colors[:len(box_plot['boxes'])]):\n",
    "        patch.set_facecolor(color)\n",
    "    ax.set_title('Parasitemia Scores by Infection Level')\n",
    "    ax.set_ylabel('Parasitemia Score (%)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Cell count distribution\n",
    "    ax = axes[1, 1]\n",
    "    cell_types = ['Infected', 'Healthy', 'WBC']\n",
    "    cell_counts = [total_infected, total_healthy, total_wbc]\n",
    "    colors = ['red', 'blue', 'purple']\n",
    "    bars = ax.bar(cell_types, cell_counts, color=colors, alpha=0.7)\n",
    "    ax.set_title('Total Cell Counts by Type')\n",
    "    ax.set_ylabel('Cell Count')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for bar, count in zip(bars, cell_counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count:,}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_images': len(dataset_records),\n",
    "        'level_distribution': level_counts,\n",
    "        'score_stats': {\n",
    "            'mean': np.mean(parasitemia_scores),\n",
    "            'std': np.std(parasitemia_scores),\n",
    "            'min': np.min(parasitemia_scores),\n",
    "            'max': np.max(parasitemia_scores),\n",
    "            'median': np.median(parasitemia_scores)\n",
    "        },\n",
    "        'cell_totals': {\n",
    "            'infected': total_infected,\n",
    "            'healthy': total_healthy,\n",
    "            'wbc': total_wbc,\n",
    "            'total': total_annotations\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0e35a",
   "metadata": {},
   "source": [
    "### 2.6.2 Resized Annotation Lookup Creation\n",
    "*Create lookup tables for resized annotations to preserve spatial relationships*\n",
    "\n",
    "### 2.6.3 Final Dataset Analysis and Visualization\n",
    "*Comprehensive analysis of the created dataset*\n",
    "\n",
    "**Analysis Components:**\n",
    "- **Size Distribution**: Images per infection level\n",
    "- **Parasitemia Statistics**: Mean, std, min, max, median\n",
    "- **Cell Count Analysis**: Total infected, healthy, WBC counts\n",
    "- **Visual Distribution**: Pie charts, histograms, box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1434f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_dataset(dataset_records, dataset_stats):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAVING FINAL DATASET\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # dirs\n",
    "    images_dir = os.path.join(OUTPUT_DIR, 'processed_images')\n",
    "    metadata_dir = os.path.join(OUTPUT_DIR, 'metadata')\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    \n",
    "    # Save processed images and create metadata\n",
    "    metadata_records = []\n",
    "    \n",
    "    print(\"Saving processed images...\")\n",
    "    for i, record in enumerate(tqdm(dataset_records, desc=\"Saving images\")):\n",
    "        \n",
    "        # Save processed image\n",
    "        img_filename = f\"{record['image_id']}.png\"\n",
    "        img_path = os.path.join(images_dir, img_filename)\n",
    "        img_pil = Image.fromarray(record['processed_image'])\n",
    "        img_pil.save(img_path, 'PNG')\n",
    "        \n",
    "        # Create metadata record (without the actual image data)\n",
    "        metadata_record = {k: v for k, v in record.items() if k != 'processed_image'}\n",
    "        metadata_record['processed_image_path'] = img_path\n",
    "        metadata_records.append(metadata_record)\n",
    "    \n",
    "    # Save metadata as JSON\n",
    "    metadata_path = os.path.join(metadata_dir, 'dataset_metadata.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata_records, f, indent=2, default=str)\n",
    "    \n",
    "    # Save dataset statistics\n",
    "    stats_path = os.path.join(metadata_dir, 'dataset_statistics.json')\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f, indent=2, default=str)\n",
    "    \n",
    "    # Create CSV for easy analysis\n",
    "    csv_data = []\n",
    "    for record in metadata_records:\n",
    "        csv_row = {\n",
    "            'image_id': record['image_id'],\n",
    "            'image_name': record['image_name'],\n",
    "            'parasitemia_score': record['parasitemia_score'],\n",
    "            'infection_level': record['infection_level'],\n",
    "            'infection_category': record['infection_category'],\n",
    "            'infected_cells': record['cell_counts']['infected'],\n",
    "            'healthy_cells': record['cell_counts']['healthy'],\n",
    "            'wbc_cells': record['cell_counts']['wbc'],\n",
    "            'total_annotations': record['cell_counts']['total_annotations'],\n",
    "            'preprocessing_method': record['preprocessing_method'],\n",
    "            'scoring_method': record['scoring_method']\n",
    "        }\n",
    "        csv_data.append(csv_row)\n",
    "    \n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_path = os.path.join(metadata_dir, 'dataset_summary.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nDataset saved successfully!\")\n",
    "    print(f\"  Images: {images_dir}\")\n",
    "    print(f\"  Metadata: {metadata_path}\")\n",
    "    print(f\"  Statistics: {stats_path}\")\n",
    "    print(f\"  CSV Summary: {csv_path}\")\n",
    "    print(f\"  Total files: {len(dataset_records)} images + metadata\")\n",
    "    \n",
    "    return {\n",
    "        'images_dir': images_dir,\n",
    "        'metadata_path': metadata_path,\n",
    "        'csv_path': csv_path,\n",
    "        'stats_path': stats_path\n",
    "    }\n",
    "\n",
    "# Dataset creation\n",
    "print(\"Starting final dataset creation pipeline...\")\n",
    "\n",
    "# Get the best methods\n",
    "if 'best_method_parasitemia' not in locals():\n",
    "    best_method_parasitemia = 'Count-based'  \n",
    "if 'best_method_preprocessing' not in locals():\n",
    "    best_method_preprocessing = 'CLAHE'  \n",
    "\n",
    "print(f\"Using parasitemia method: {best_method_parasitemia}\")\n",
    "print(f\"Using preprocessing method: {best_method_preprocessing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d322b91",
   "metadata": {},
   "source": [
    "### 2.6.4 Dataset Saving and Metadata Creation\n",
    "*Save processed dataset with comprehensive metadata*\n",
    "\n",
    "**Saved Components:**\n",
    "- **Processed Images**: Enhanced microscopy images (PNG format)\n",
    "- **Metadata JSON**: Complete dataset information\n",
    "- **Statistics JSON**: Dataset statistics and distributions  \n",
    "- **CSV Summary**: Tabular data for easy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset\n",
    "final_dataset = create_final_dataset(best_method_parasitemia, all_parasitemia_results, best_method_preprocessing)\n",
    "\n",
    "# Analyze dataset\n",
    "dataset_stats = analyze_final_dataset(final_dataset)\n",
    "\n",
    "# Save everything\n",
    "save_paths = save_final_dataset(final_dataset, dataset_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET CREATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final dataset contains {len(final_dataset)} processed images\")\n",
    "print(f\"Ready for MTTL training pipeline\")\n",
    "print(f\"All files saved to: {os.path.abspath(OUTPUT_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffbc26",
   "metadata": {},
   "source": [
    "## 2.7 Training Data Preparation (Multi-Task & Single-Task Compatible)\n",
    "\n",
    "### 2.7.1 Training Pipeline Setup\n",
    "*Set up the training data preparation pipeline with PyTorch integration*\n",
    "\n",
    "### 2.7.2 Multi-Task Data Preparation\n",
    "*Prepare data for all three MTTL tasks simultaneously*\n",
    "\n",
    "**Task Preparation:**\n",
    "1. **Detection Task**: Bounding box annotation processing\n",
    "2. **Regression Task**: Parasitemia score normalization  \n",
    "3. **Localization Task**: Infection-only heatmap generation\n",
    "\n",
    "### 2.7.3 Train/Validation/Test Split Creation\n",
    "*Create stratified splits maintaining infection level distribution*\n",
    "\n",
    "**Split Strategy:**\n",
    "- **Training**: 70% (stratified by infection level)\n",
    "- **Validation**: 15% (stratified by infection level)\n",
    "- **Testing**: 15% (stratified by infection level)\n",
    "\n",
    "### 2.7.4 Task Mode Testing\n",
    "*Verify all MTTL task modes work correctly with the dataset*\n",
    "\n",
    "**Task Modes Tested:**\n",
    "- **Detection Mode**: Object detection with bounding boxes\n",
    "- **Regression Mode**: Parasitemia score prediction\n",
    "- **Localization Mode**: Infection heatmap generation\n",
    "- **Multi-Task Mode**: All tasks combined\n",
    "\n",
    "### 2.7.5 Infection Localization Heatmap Visualization\n",
    "*Visualize and analyze the infection-only heatmap generation*\n",
    "\n",
    "**Heatmap Features:**\n",
    "- **Infection-Only Focus**: Only infected regions highlighted (red colormap)\n",
    "- **Spatial Accuracy**: Precise infection localization\n",
    "- **Intensity Mapping**: Variable intensity based on infection density\n",
    "- **Clinical Relevance**: Direct visual feedback for diagnosis\n",
    "\n",
    "### 2.7.6 Heatmap Analysis by Infection Level\n",
    "*Comprehensive analysis of heatmap quality across different parasitemia levels*\n",
    "\n",
    "**Analysis Categories:**\n",
    "- **Negative (0%)**: No infection heatmap (validation)\n",
    "- **Low (0-2%)**: Sparse infection patterns\n",
    "- **Moderate (2-10%)**: Moderate infection coverage\n",
    "- **High (>10%)**: Dense infection patterns\n",
    "\n",
    "### 2.7.7 Comprehensive Heatmap Statistical Analysis\n",
    "*Quantitative analysis of infection heatmap characteristics*\n",
    "\n",
    "**Statistical Metrics:**\n",
    "- **Coverage Percentage**: Infection pixel ratio\n",
    "- **Intensity Statistics**: Max, mean, standard deviation\n",
    "- **Precision Analysis**: Pixels per infected cell ratio\n",
    "- **Spatial Distribution**: Infection pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f41340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Prepare Data for Training (Multi-Task & Single-Task Compatible) ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARING DATA FOR TRAINING - MULTI-TASK & SINGLE-TASK COMPATIBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "#import sys\n",
    "#import os\n",
    "#sys.path.append('..')\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook')\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from src.utils.DataUtils import MalariaDataPreparator, FlexibleMalariaDataset, set_seeds\n",
    "SEED = 12\n",
    "set_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_annotations_on_image(ax, image, annotations):\n",
    "    if image.dtype != np.uint8:\n",
    "        if image.max() <= 1.0:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        else:\n",
    "            image = np.clip(image, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    ax.set_xlim(0, image.shape[1])\n",
    "    ax.set_ylim(image.shape[0], 0)\n",
    "    \n",
    "    CELL_TYPE_COLORS = {\n",
    "        'parasitized': 'red',\n",
    "        'Parasitized': 'red',\n",
    "        'uninfected': 'lime', \n",
    "        'Uninfected': 'lime',\n",
    "        'white_blood_cell': 'blue',\n",
    "        'White_Blood_Cell': 'blue'\n",
    "    }\n",
    "    \n",
    "    CELL_TYPE_SHORT = {\n",
    "        'parasitized': 'P',\n",
    "        'Parasitized': 'P', \n",
    "        'uninfected': 'U',\n",
    "        'Uninfected': 'U',\n",
    "        'white_blood_cell': 'W',\n",
    "        'White_Blood_Cell': 'W'\n",
    "    }\n",
    "    \n",
    "    for ann in annotations:\n",
    "        cell_type = ann.get('cell_type', 'unknown')\n",
    "        color = CELL_TYPE_COLORS.get(cell_type, 'yellow')\n",
    "        short_label = CELL_TYPE_SHORT.get(cell_type, '?')\n",
    "        \n",
    "        if ann.get('shape') == 'Point':\n",
    "            x, y = ann.get('x', 0), ann.get('y', 0)\n",
    "            x = max(0, min(x, image.shape[1]-1))\n",
    "            y = max(0, min(y, image.shape[0]-1))\n",
    "            \n",
    "            ax.plot(x, y, 'o', color=color, markersize=6, markeredgewidth=1.5, markeredgecolor='black')\n",
    "            \n",
    "            text_x = min(x + 5, image.shape[1] - 15)\n",
    "            text_y = max(y - 5, 15)\n",
    "            \n",
    "            ax.text(text_x, text_y, short_label, color=color, fontsize=8, weight='bold',\n",
    "                   bbox=dict(facecolor='white', alpha=0.8, edgecolor=color, boxstyle='round,pad=0.2'))\n",
    "        \n",
    "        elif ann.get('shape') == 'Polygon' and 'polygon' in ann:\n",
    "            coords = ann['polygon']\n",
    "            clipped_coords = []\n",
    "            for x, y in coords:\n",
    "                x = max(0, min(x, image.shape[1]-1))\n",
    "                y = max(0, min(y, image.shape[0]-1))\n",
    "                clipped_coords.append((x, y))\n",
    "            \n",
    "            if clipped_coords:\n",
    "                poly = np.array(clipped_coords)\n",
    "                patch = patches.Polygon(poly, closed=True, fill=False, edgecolor=color, linewidth=1.5)\n",
    "                ax.add_patch(patch)\n",
    "                \n",
    "                centroid_x = np.mean([p[0] for p in clipped_coords])\n",
    "                centroid_y = np.mean([p[1] for p in clipped_coords])\n",
    "                \n",
    "                ax.text(centroid_x, centroid_y, short_label, color=color, fontsize=8, weight='bold',\n",
    "                       bbox=dict(facecolor='white', alpha=0.8, edgecolor=color, boxstyle='round,pad=0.2'),\n",
    "                       ha='center', va='center')\n",
    "    \n",
    "    ax.axis('off')\n",
    "\n",
    "def create_train_val_test_split(training_samples, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=SEED):\n",
    "    print(f\"Creating train/val/test split...\")\n",
    "    print(f\"Split ratios: train={train_ratio}, val={val_ratio}, test={test_ratio}\")\n",
    "    \n",
    "    score_groups = defaultdict(list)\n",
    "    for i, sample in enumerate(training_samples):\n",
    "        score = sample['regression']['parasitemia_score']\n",
    "        if score == 0:\n",
    "            group = 'negative'\n",
    "        elif score <= 2:\n",
    "            group = 'low'\n",
    "        elif score <= 10:\n",
    "            group = 'moderate'\n",
    "        else:\n",
    "            group = 'high'\n",
    "        score_groups[group].append(i)\n",
    "    \n",
    "    train_indices, val_indices, test_indices = [], [], []\n",
    "    \n",
    "    for group, indices in score_groups.items():\n",
    "        n_samples = len(indices)\n",
    "        n_train = int(n_samples * train_ratio)\n",
    "        n_val = int(n_samples * val_ratio)\n",
    "        n_test = n_samples - n_train - n_val\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        train_indices.extend(indices[:n_train])\n",
    "        val_indices.extend(indices[n_train:n_train+n_val])\n",
    "        test_indices.extend(indices[n_train+n_val:])\n",
    "        \n",
    "        print(f\"  {group:10}: {n_samples:3d} total -> train: {n_train:3d}, val: {n_val:3d}, test: {n_test:3d}\")\n",
    "    \n",
    "    train_samples = [training_samples[i] for i in train_indices]\n",
    "    val_samples = [training_samples[i] for i in val_indices]\n",
    "    test_samples = [training_samples[i] for i in test_indices]\n",
    "    \n",
    "    print(f\"Final split sizes:\")\n",
    "    print(f\"Train: {len(train_samples):3d} samples ({len(train_samples)/len(training_samples)*100:.1f}%)\")\n",
    "    print(f\"Val:   {len(val_samples):3d} samples ({len(val_samples)/len(training_samples)*100:.1f}%)\")\n",
    "    print(f\"Test:  {len(test_samples):3d} samples ({len(test_samples)/len(training_samples)*100:.1f}%)\")\n",
    "    \n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "def test_task_modes(train_samples):\n",
    "    print(\"TESTING MTTL TASK MODES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    task_modes = ['detection', 'regression', 'localization', 'multi_task', 'segmentation', 'severity']\n",
    "    \n",
    "    for task_mode in task_modes:\n",
    "        print(f\"\\nTesting {task_mode.upper()} mode...\")\n",
    "        \n",
    "        try:\n",
    "            dataset = FlexibleMalariaDataset(train_samples[:5], task_mode=task_mode, augment=False)\n",
    "            \n",
    "            if task_mode in ['detection', 'multi_task']:\n",
    "                def collate_fn(batch):\n",
    "                    return batch\n",
    "                dataloader = DataLoader(dataset, batch_size=2, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "            else:\n",
    "                dataloader = DataLoader(dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "            \n",
    "            sample_batch = next(iter(dataloader))\n",
    "            \n",
    "            print(f\"Success: {task_mode} mode working!\")\n",
    "            \n",
    "            # Samples inspection\n",
    "            if isinstance(sample_batch, list):\n",
    "                print(f\"Batch type: List of {len(sample_batch)} samples\")\n",
    "                sample = sample_batch[0]  \n",
    "                print(f\"Sample keys: {list(sample.keys())}\")\n",
    "                \n",
    "                # shapes and types\n",
    "                for key, value in sample.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        print(f\"{key:15}: Tensor {tuple(value.shape)} | dtype: {value.dtype} | range: [{value.min():.3f}, {value.max():.3f}]\")\n",
    "                    elif isinstance(value, (int, float)):\n",
    "                        print(f\"{key:15}: {type(value).__name__} | value: {value}\")\n",
    "                    else:\n",
    "                        print(f\"{key:15}: {type(value).__name__} | value: {value}\")\n",
    "                        \n",
    "            else:\n",
    "                print(f\"Batch type: Single tensor batch\")\n",
    "                print(f\"Batch keys: {list(sample_batch.keys())}\")\n",
    "                \n",
    "                # Show detailed shapes and types\n",
    "                for key, value in sample_batch.items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        if len(value.shape) > 1:  \n",
    "                            print(f\"{key:15}: Tensor {tuple(value.shape)} | dtype: {value.dtype} | range: [{value.min():.3f}, {value.max():.3f}]\")\n",
    "                        else:  \n",
    "                            print(f\"{key:15}: Tensor {tuple(value.shape)} | dtype: {value.dtype} | values: {value.tolist()}\")\n",
    "                    elif isinstance(value, list):\n",
    "                        print(f\"{key:15}: List of {len(value)} items | sample: {value[:3] if len(value) > 3 else value}\")\n",
    "                    else:\n",
    "                        print(f\"{key:15}: {type(value).__name__} | value: {value}\")\n",
    "            \n",
    "            # Task-specific info\n",
    "            if task_mode == 'detection':\n",
    "                sample = sample_batch[0] if isinstance(sample_batch, list) else sample_batch\n",
    "                num_boxes = len(sample['bboxes']) if len(sample['bboxes'].shape) > 1 else 1 if sample['bboxes'].numel() > 0 else 0\n",
    "                print(f\"Detection info: {num_boxes} bounding boxes detected\")\n",
    "                \n",
    "            elif task_mode == 'multi_task':\n",
    "                sample = sample_batch[0] if isinstance(sample_batch, list) else sample_batch\n",
    "                num_boxes = len(sample['bboxes']) if len(sample['bboxes'].shape) > 1 else 1 if sample['bboxes'].numel() > 0 else 0\n",
    "                heatmap_coverage = (sample['heatmap'] > 0).sum().item() / sample['heatmap'].numel() * 100\n",
    "                print(f\"Multi-task info: {num_boxes} boxes, {heatmap_coverage:.1f}% heatmap coverage\")\n",
    "                \n",
    "            elif task_mode == 'localization':\n",
    "                heatmap = sample_batch['heatmap'][0] if len(sample_batch['heatmap'].shape) > 2 else sample_batch['heatmap']\n",
    "                coverage = (heatmap > 0).sum().item() / heatmap.numel() * 100\n",
    "                print(f\"Localization info: {coverage:.1f}% infection coverage\")\n",
    "                \n",
    "            elif task_mode == 'regression':\n",
    "                scores = sample_batch['parasitemia_score'] if len(sample_batch['parasitemia_score'].shape) > 0 else [sample_batch['parasitemia_score']]\n",
    "                print(f\"Regression info: parasitemia scores range [{scores.min():.2f}, {scores.max():.2f}]\")\n",
    "                \n",
    "            elif task_mode == 'segmentation':\n",
    "                mask = sample_batch['mask'] if isinstance(sample_batch, dict) else sample_batch[0]['mask']\n",
    "                print(f\"Segmentation info: mask shape {mask.shape}, unique values: {np.unique(mask)}\")\n",
    "                \n",
    "            elif task_mode == 'severity':\n",
    "                severity_class = sample_batch['severity_class'] if isinstance(sample_batch, dict) else sample_batch[0]['severity_class']\n",
    "                severity_label = sample_batch['severity_label'] if isinstance(sample_batch, dict) else sample_batch[0]['severity_label']\n",
    "                print(f\"Severity info: class={severity_class}, label={severity_label}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {task_mode} mode failed: {str(e)}\")\n",
    "            import traceback\n",
    "            print(f\"Full error: {traceback.format_exc()}\")\n",
    "            \n",
    "def visualize_heatmaps(samples, num_samples=4):\n",
    "    print(\"VISUALIZING...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, num_samples, figsize=(num_samples*5, 18), gridspec_kw={'hspace': 0.3, 'wspace': 0.15})\n",
    "    fig.suptitle('Infection-Only Localization Heatmaps', fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    selected_samples = np.random.choice(len(samples), min(num_samples, len(samples)), replace=False)\n",
    "    severity_labels = []\n",
    "    \n",
    "    for i, idx in enumerate(selected_samples):\n",
    "        sample = samples[idx]\n",
    "        image = sample['image'].copy()\n",
    "        if image.dtype != np.uint8:\n",
    "            if image.max() <= 1.0:\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "            else:\n",
    "                image = np.clip(image, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        record_id = sample['image_id']\n",
    "        resized_annotations = []\n",
    "        for record in final_dataset:\n",
    "            if record['image_id'] == record_id:\n",
    "                resized_annotations = record.get('resized_annotations', [])\n",
    "                break\n",
    "        \n",
    "        # Row 0: Original + Annotations\n",
    "        ax = axes[0, i]\n",
    "        draw_annotations_on_image(ax, image, resized_annotations)\n",
    "        infected_count = sample['cell_counts']['infected']\n",
    "        healthy_count = sample['cell_counts']['healthy']\n",
    "        ax.set_title(f\"Original + Annotations\\nInfected: {infected_count}, Healthy: {healthy_count}\", fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Row 1: Infection Heatmap\n",
    "        ax = axes[1, i]\n",
    "        heatmap = sample['localization']['heatmap']\n",
    "        im = ax.imshow(heatmap, cmap='Reds', interpolation='bilinear', aspect='equal', vmin=0, vmax=1)\n",
    "        ax.set_title(f\"Infection Heatmap\\nRange: {heatmap.min():.3f} - {heatmap.max():.3f}\", fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        # Only add colorbar to the last heatmap\n",
    "        if i == num_samples - 1:\n",
    "            cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04, shrink=0.8)\n",
    "            cbar.set_label('Infection Intensity', rotation=270, labelpad=20, fontsize=11)\n",
    "            tick_positions = [0, 0.3, 0.5, 0.7, 1.0]\n",
    "            tick_labels = ['None', 'Low', 'Medium', 'High', 'Critical']\n",
    "            cbar.set_ticks(tick_positions)\n",
    "            cbar.set_ticklabels(tick_labels, fontsize=9)\n",
    "        \n",
    "        # Row 2: Combined View\n",
    "        ax = axes[2, i]\n",
    "        ax.imshow(image, alpha=0.7)\n",
    "        ax.imshow(heatmap, cmap='Reds', alpha=0.6, interpolation='bilinear')\n",
    "        parasitemia = sample['regression']['parasitemia_score']\n",
    "        ax.set_title(f\"Combined View\\nParasitemia: {parasitemia:.1f}%\", fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Row 3: Binary Segmentation Mask\n",
    "        ax = axes[3, i]\n",
    "        mask = sample['segmentation']['mask'] if 'segmentation' in sample else sample['multi_task']['mask']\n",
    "        ax.imshow(mask, cmap='gray', interpolation='nearest', vmin=0, vmax=1)\n",
    "        ax.set_title(\"Binary Segmentation Mask\\n(1=cell, 0=background)\", fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Collect severity for bar plot\n",
    "        severity = sample['severity']['severity_label'] if 'severity' in sample else sample['multi_task']['severity_label']\n",
    "        severity_labels.append(severity)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    \n",
    "    # Add severity bar plot below\n",
    "    plt.figure(figsize=(num_samples*5, 2.5))\n",
    "    severity_map = {'negative': 0, 'low': 1, 'moderate': 2, 'high': 3}\n",
    "    severity_numeric = [severity_map.get(s, -1) for s in severity_labels]\n",
    "    severity_colors = ['green', 'yellow', 'orange', 'red']\n",
    "    plt.bar(range(num_samples), severity_numeric, color=[severity_colors[s] if s >= 0 else 'gray' for s in severity_numeric])\n",
    "    plt.xticks(range(num_samples), [f\"Sample {i+1}\" for i in range(num_samples)], fontsize=12)\n",
    "    plt.yticks([0,1,2,3], ['Negative', 'Low', 'Moderate', 'High'], fontsize=12)\n",
    "    plt.title('Severity Level per Sample', fontsize=15, fontweight='bold')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Severity')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_heatmaps_by_infection_level(training_samples, samples_per_level=3):\n",
    "    print(\"INFECTION-ONLY HEATMAP BY PARASITEMIA LEVELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    infection_categories = {\n",
    "        'Negative (0%)': [],\n",
    "        'Low (0-2%)': [],\n",
    "        'Moderate (2-10%)': [],\n",
    "        'High (>10%)': []\n",
    "    }\n",
    "    \n",
    "    for sample in training_samples:\n",
    "        score = sample['regression']['parasitemia_score']\n",
    "        if score == 0:\n",
    "            infection_categories['Negative (0%)'].append(sample)\n",
    "        elif score <= 2:\n",
    "            infection_categories['Low (0-2%)'].append(sample)\n",
    "        elif score <= 10:\n",
    "            infection_categories['Moderate (2-10%)'].append(sample)\n",
    "        else:\n",
    "            infection_categories['High (>10%)'].append(sample)\n",
    "    \n",
    "    print(\"INFECTION LEVEL DISTRIBUTION:\")\n",
    "    for level, samples in infection_categories.items():\n",
    "        print(f\"  {level:15}: {len(samples):3d} samples\")\n",
    "    \n",
    "    fig, axes = plt.subplots(4, samples_per_level, figsize=(samples_per_level*5, 20))\n",
    "    fig.suptitle('Infection-Only Localization Heatmaps by Parasitemia Level', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    row_titles = ['Negative (0%)', 'Low (0-2%)', 'Moderate (2-10%)', 'High (>10%)']\n",
    "    \n",
    "    for row, (level, samples) in enumerate(infection_categories.items()):\n",
    "        if len(samples) == 0:\n",
    "            for col in range(samples_per_level):\n",
    "                axes[row, col].text(0.5, 0.5, f'No {level} samples', \n",
    "                                  transform=axes[row, col].transAxes,\n",
    "                                  ha='center', va='center', fontsize=12)\n",
    "                axes[row, col].axis('off')\n",
    "            continue\n",
    "        \n",
    "        selected_indices = np.random.choice(len(samples), \n",
    "                                          min(samples_per_level, len(samples)), \n",
    "                                          replace=False)\n",
    "        \n",
    "        for col, idx in enumerate(selected_indices):\n",
    "            sample = samples[idx]\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            image = sample['image'].copy()\n",
    "            if image.dtype != np.uint8:\n",
    "                if image.max() <= 1.0:\n",
    "                    image = (image * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    image = np.clip(image, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            heatmap = sample['localization']['heatmap']\n",
    "            parasitemia = sample['regression']['parasitemia_score']\n",
    "            infected_count = sample['cell_counts']['infected']\n",
    "            healthy_count = sample['cell_counts']['healthy']\n",
    "            \n",
    "            # Show original image\n",
    "            ax.imshow(image, alpha=0.6)\n",
    "            \n",
    "            # Only show heatmap where there are infected regions\n",
    "            if np.any(heatmap > 0):\n",
    "                im = ax.imshow(heatmap, cmap='Reds', alpha=0.8, \n",
    "                             interpolation='bilinear', vmin=0, vmax=1)\n",
    "            \n",
    "            if col == samples_per_level - 1:\n",
    "                if np.any(heatmap > 0):  #add colorbar if there are infections\n",
    "                    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04, shrink=0.8)\n",
    "                    cbar.set_label('Infection', rotation=270, \n",
    "                                 labelpad=15, fontsize=10)\n",
    "                    \n",
    "                    tick_positions = [0, 0.5, 1.0]\n",
    "                    tick_labels = ['None', 'Medium', 'High']\n",
    "                    cbar.set_ticks(tick_positions)\n",
    "                    cbar.set_ticklabels(tick_labels, fontsize=8)\n",
    "            \n",
    "            coverage = np.sum(heatmap > 0) / heatmap.size * 100\n",
    "            ax.set_title(f'Score: {parasitemia:.1f}%\\nInfected: {infected_count}\\nCoverage: {coverage:.1f}%', \n",
    "                        fontsize=9, pad=8)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        for col in range(len(selected_indices), samples_per_level):\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        axes[row, 0].text(-0.15, 0.5, row_titles[row], \n",
    "                         transform=axes[row, 0].transAxes,\n",
    "                         rotation=90, ha='center', va='center',\n",
    "                         fontsize=14, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0.1, top=0.95)\n",
    "    plt.show()\n",
    "\n",
    "def create_comprehensive_analysis(training_samples):\n",
    "    print(\"COMPREHENSIVE INFECTION-ONLY HEATMAP ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    analysis_data = []\n",
    "    \n",
    "    for sample in training_samples:\n",
    "        score = sample['regression']['parasitemia_score']\n",
    "        heatmap = sample['localization']['heatmap']\n",
    "        \n",
    "        if score == 0:\n",
    "            level = 'Negative'\n",
    "        elif score <= 2:\n",
    "            level = 'Low'\n",
    "        elif score <= 10:\n",
    "            level = 'Moderate'\n",
    "        else:\n",
    "            level = 'High'\n",
    "        \n",
    "        # Only analyze infection pixels\n",
    "        infection_pixels = np.sum(heatmap > 0)\n",
    "        background_pixels = np.sum(heatmap == 0)\n",
    "        precision = infection_pixels / max(sample['cell_counts']['infected'], 1)\n",
    "        \n",
    "        analysis_data.append({\n",
    "            'level': level,\n",
    "            'parasitemia_score': score,\n",
    "            'max_intensity': heatmap.max(),\n",
    "            'mean_intensity': heatmap.mean(),\n",
    "            'std_intensity': heatmap.std(),\n",
    "            'infection_pixels': infection_pixels,\n",
    "            'background_pixels': background_pixels,\n",
    "            'coverage_percent': infection_pixels / heatmap.size * 100,\n",
    "            'precision_pixels_per_cell': precision,\n",
    "            'infected_count': sample['cell_counts']['infected'],\n",
    "            'healthy_count': sample['cell_counts']['healthy']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    summary = df.groupby('level').agg({\n",
    "        'parasitemia_score': ['mean', 'std', 'count'],\n",
    "        'max_intensity': ['mean', 'std'],\n",
    "        'mean_intensity': ['mean', 'std'],\n",
    "        'coverage_percent': ['mean', 'std'],\n",
    "        'precision_pixels_per_cell': ['mean', 'std'],\n",
    "        'infected_count': ['mean', 'std'],\n",
    "        'healthy_count': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"STATISTICAL SUMMARY BY INFECTION LEVEL:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Infection-Only Heatmap Statistics by Infection Level', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    df.boxplot(column='max_intensity', by='level', ax=ax)\n",
    "    ax.set_title('Maximum Infection Intensity')\n",
    "    ax.set_xlabel('Infection Level')\n",
    "    ax.set_ylabel('Max Intensity')\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    df.boxplot(column='precision_pixels_per_cell', by='level', ax=ax)\n",
    "    ax.set_title('Precision (Pixels per Infected Cell)')\n",
    "    ax.set_xlabel('Infection Level')\n",
    "    ax.set_ylabel('Precision (px/cell)')\n",
    "    \n",
    "    ax = axes[0, 2]\n",
    "    df.boxplot(column='coverage_percent', by='level', ax=ax)\n",
    "    ax.set_title('Infection Coverage Percentage')\n",
    "    ax.set_xlabel('Infection Level')\n",
    "    ax.set_ylabel('Coverage (%)')\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    level_means = df.groupby('level')[['infection_pixels', 'background_pixels']].mean()\n",
    "    level_means.plot(kind='bar', ax=ax, stacked=True, color=[\"#F32323\", \"#8C17F2\"])\n",
    "    ax.set_title('Average Pixel Distribution')\n",
    "    ax.set_xlabel('Infection Level')\n",
    "    ax.set_ylabel('Average Pixel Count')\n",
    "    ax.legend(['Infection (Red)', 'Background (Black)'])\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    colors = {'Negative': 'green', 'Low': 'yellow', 'Moderate': 'orange', 'High': 'red'}\n",
    "    for level in df['level'].unique():\n",
    "        level_data = df[df['level'] == level]\n",
    "        ax.scatter(level_data['parasitemia_score'], level_data['precision_pixels_per_cell'], \n",
    "                  c=colors.get(level, 'blue'), label=level, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Parasitemia Score (%)')\n",
    "    ax.set_ylabel('Precision (px/cell)')\n",
    "    ax.set_title('Parasitemia vs Heatmap Precision')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 2]\n",
    "    for level in df['level'].unique():\n",
    "        level_data = df[df['level'] == level]\n",
    "        ax.scatter(level_data['healthy_count'], level_data['infected_count'], \n",
    "                  c=colors.get(level, 'blue'), label=level, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Healthy Cell Count')\n",
    "    ax.set_ylabel('Infected Cell Count')\n",
    "    ax.set_title('Healthy vs Infected Cell Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a69d73f",
   "metadata": {},
   "source": [
    "### 2.7.8 Final Training Dataset Creation and Saving\n",
    "*Save the complete training-ready dataset with all task components*\n",
    "\n",
    "**Dataset Structure:**\n",
    "```\n",
    "mttl_training_data/\n",
    "├── train/\n",
    "│   ├── images/          # Processed images (PNG)\n",
    "│   ├── heatmaps/        # Infection heatmaps (NPY)\n",
    "│   ├── train_metadata.json\n",
    "│   └── train_samples.pkl\n",
    "├── val/\n",
    "│   ├── images/\n",
    "│   ├── heatmaps/\n",
    "│   ├── val_metadata.json\n",
    "│   └── val_samples.pkl\n",
    "├── test/\n",
    "│   ├── images/\n",
    "│   ├── heatmaps/\n",
    "│   ├── test_metadata.json\n",
    "│   └── test_samples.pkl\n",
    "└── dataset_info.json   # Complete dataset information\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_training_dataset(train_samples, val_samples, test_samples, save_dir):\n",
    "    print(f\"Saving training dataset to {save_dir}...\")\n",
    "    \n",
    "    train_dir = os.path.join(save_dir, 'train')\n",
    "    val_dir = os.path.join(save_dir, 'val')\n",
    "    test_dir = os.path.join(save_dir, 'test')\n",
    "    \n",
    "    for dir_path in [train_dir, val_dir, test_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        os.makedirs(os.path.join(dir_path, 'images'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(dir_path, 'heatmaps'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(dir_path, 'masks'), exist_ok=True)\n",
    "\n",
    "    def save_split(samples, split_dir, split_name):\n",
    "        print(f\"  Saving {split_name} split ({len(samples)} samples)...\")\n",
    "        \n",
    "        for sample in tqdm(samples, desc=f\"Saving {split_name}\"):\n",
    "            img_path = os.path.join(split_dir, 'images', f\"{sample['image_id']}.png\")\n",
    "            img_pil = Image.fromarray(sample['image'])\n",
    "            img_pil.save(img_path)\n",
    "            \n",
    "            heatmap_path = os.path.join(split_dir, 'heatmaps', f\"{sample['image_id']}.npy\")\n",
    "            np.save(heatmap_path, sample['localization']['heatmap'])\n",
    "            \n",
    "            mask_path = os.path.join(split_dir, 'masks', f\"{sample['image_id']}.npy\")\n",
    "            np.save(mask_path, sample['segmentation']['mask'])\n",
    "        \n",
    "        metadata = []\n",
    "        for sample in samples:\n",
    "            meta = {\n",
    "                'image_id': sample['image_id'],\n",
    "                'detection': {\n",
    "                    'bboxes': sample['detection']['bboxes'].tolist() if len(sample['detection']['bboxes']) > 0 else [],\n",
    "                    'labels': sample['detection']['labels'].tolist() if len(sample['detection']['labels']) > 0 else [],\n",
    "                    'num_objects': sample['detection']['num_objects']\n",
    "                },\n",
    "                'regression': {\n",
    "                    'parasitemia_score': float(sample['regression']['parasitemia_score'])\n",
    "                },\n",
    "                'localization': {\n",
    "                    'heatmap_path': f\"heatmaps/{sample['image_id']}.npy\",\n",
    "                    'max_intensity': float(sample['localization']['heatmap'].max())\n",
    "                },\n",
    "                'segmentation': {\n",
    "                    'mask_path': f\"masks/{sample['image_id']}.npy\",\n",
    "                    'mask_shape': list(sample['segmentation']['mask'].shape)\n",
    "                },\n",
    "                'severity': {\n",
    "                    'severity_class': sample['severity']['severity_class'],\n",
    "                    'severity_label': sample['severity']['severity_label']\n",
    "                },\n",
    "                'multi_task': {\n",
    "                    'parasitemia_score': float(sample['multi_task']['parasitemia_score']),\n",
    "                    'bboxes': sample['multi_task']['bboxes'].tolist() if len(sample['multi_task']['bboxes']) > 0 else [],\n",
    "                    'bbox_labels': sample['multi_task']['bbox_labels'].tolist() if len(sample['multi_task']['bbox_labels']) > 0 else [],\n",
    "                    'mask_path': f\"masks/{sample['image_id']}.npy\",  \n",
    "                    'mask_shape': list(sample['segmentation']['mask'].shape),  \n",
    "                    'severity_label': sample['severity']['severity_label']  \n",
    "                },\n",
    "                'cell_counts': sample['cell_counts'],\n",
    "                'metadata': sample['metadata']\n",
    "            }\n",
    "            metadata.append(meta)\n",
    "        \n",
    "        with open(os.path.join(split_dir, f'{split_name}_metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        with open(os.path.join(split_dir, f'{split_name}_samples.pkl'), 'wb') as f:\n",
    "            pickle.dump(samples, f)\n",
    "    \n",
    "    save_split(train_samples, train_dir, 'train')\n",
    "    save_split(val_samples, val_dir, 'val')\n",
    "    save_split(test_samples, test_dir, 'test')\n",
    "    \n",
    "    # Get actual info\n",
    "    all_samples = train_samples + val_samples + test_samples\n",
    "    \n",
    "    # Get actual image size from first sample\n",
    "    actual_image_size = list(all_samples[0]['image'].shape)\n",
    "    \n",
    "    # Get actual heatmap size from first sample  \n",
    "    actual_heatmap_size = list(all_samples[0]['localization']['heatmap'].shape)\n",
    "    \n",
    "    # Get actual class info from detection labels\n",
    "    all_labels = []\n",
    "    for sample in all_samples:\n",
    "        labels = sample['detection']['labels']\n",
    "        all_labels.extend(labels.tolist() if hasattr(labels, 'tolist') else labels)\n",
    "    \n",
    "    unique_labels = sorted(set(all_labels))\n",
    "    actual_num_classes = len(unique_labels)\n",
    "    \n",
    "    # Create actual class names mapping\n",
    "    label_to_name = {0: 'Infected', 1: 'Healthy', 2: 'WBC', 3: 'Unknown'}\n",
    "    actual_class_names = [label_to_name.get(label, f'Class_{label}') for label in unique_labels]\n",
    "    \n",
    "    # Get actual parasitemia score range\n",
    "    all_scores = [sample['regression']['parasitemia_score'] for sample in all_samples]\n",
    "    \n",
    "    # Count actual objects per class\n",
    "    class_counts = {label: 0 for label in unique_labels}\n",
    "    for sample in all_samples:\n",
    "        labels = sample['detection']['labels']\n",
    "        for label in (labels.tolist() if hasattr(labels, 'tolist') else labels):\n",
    "            class_counts[label] += 1\n",
    "    \n",
    "    # Create class distribution with actual names\n",
    "    class_distribution = {actual_class_names[i]: class_counts.get(unique_labels[i], 0) \n",
    "                         for i in range(len(unique_labels))}\n",
    "    \n",
    "    dataset_info = {\n",
    "        'total_samples': len(all_samples),\n",
    "        'splits': {\n",
    "            'train_samples': len(train_samples),\n",
    "            'val_samples': len(val_samples),\n",
    "            'test_samples': len(test_samples)\n",
    "        },\n",
    "        'image_size': actual_image_size,                    \n",
    "        'heatmap_size': actual_heatmap_size,               \n",
    "        'num_classes': actual_num_classes,                 \n",
    "        'class_names': actual_class_names,                 \n",
    "        'class_labels': {name: unique_labels[i] for i, name in enumerate(actual_class_names)},  \n",
    "        'class_distribution': class_distribution,           \n",
    "        'parasitemia_range': [min(all_scores), max(all_scores)],  \n",
    "        'mask_shapes': [list(sample['segmentation']['mask'].shape) for sample in all_samples],  \n",
    "        'mask_paths': [f\"masks/{sample['image_id']}.npy\" for sample in all_samples],            \n",
    "        'severity_classes': sorted(set(sample['severity']['severity_class'] for sample in all_samples)), \n",
    "        'severity_labels': sorted(set(sample['severity']['severity_label'] for sample in all_samples)),  \n",
    "        'heatmap_type': 'infection_only',\n",
    "        'supported_tasks': {\n",
    "            'detection': 'Object detection with bounding boxes',\n",
    "            'regression': 'Parasitemia score prediction',\n",
    "            'localization': 'Infection-only localization heatmaps',\n",
    "            'segmentation': 'Pixel-wise cell segmentation',\n",
    "            'severity': 'Infection severity classification',\n",
    "            'multi_task': 'All tasks combined'\n",
    "        },\n",
    "        'created_at': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'dataset_info.json'), 'w') as f:\n",
    "        json.dump(dataset_info, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Dataset saved successfully!\")\n",
    "    print(f\"  Location: {os.path.abspath(save_dir)}\")\n",
    "    print(f\"  Train: {len(train_samples)} | Val: {len(val_samples)} | Test: {len(test_samples)}\")\n",
    "    print(f\"  Actual Classes: {actual_num_classes} - {actual_class_names}\")\n",
    "    print(f\"  Class Distribution: {class_distribution}\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "def execute_complete_pipeline():\n",
    "    print(\"RUNNING FULL PIPELINE FOR MTTL DATASET PREPARATION...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    data_preparator = MalariaDataPreparator(image_size=TARGET_SIZE)\n",
    "    \n",
    "    print(\"Step 1: Preparing training data...\")\n",
    "    training_samples = data_preparator.prepare_training_data(final_dataset)\n",
    "    \n",
    "    print(\"Step 2: Creating train/val/test splits...\")\n",
    "    train_samples, val_samples, test_samples = create_train_val_test_split(training_samples)\n",
    "    \n",
    "    print(\"Step 3: Testing task modes...\")\n",
    "    test_task_modes(train_samples)\n",
    "    \n",
    "    print(\"Step 4: Visualizing infection-only heatmaps...\")\n",
    "    visualize_heatmaps(training_samples, num_samples=4)\n",
    "    \n",
    "    print(\"Step 5: Plotting heatmaps by infection level...\")\n",
    "    plot_heatmaps_by_infection_level(training_samples, samples_per_level=3)\n",
    "    \n",
    "    print(\"Step 6: Creating comprehensive analysis...\")\n",
    "    analysis_df = create_comprehensive_analysis(training_samples)\n",
    "    \n",
    "    print(\"Step 7: Saving dataset...\")\n",
    "    training_data_dir = os.path.join(OUTPUT_DIR, 'mttl_training_data')\n",
    "    dataset_info = save_training_dataset(train_samples, val_samples, test_samples, training_data_dir)\n",
    "    \n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(f\"Training data ready for MTTL at: {training_data_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'training_samples': training_samples,\n",
    "        'train_samples': train_samples,\n",
    "        'val_samples': val_samples,\n",
    "        'test_samples': test_samples,\n",
    "        'dataset_info': dataset_info,\n",
    "        'analysis_df': analysis_df\n",
    "    }\n",
    "\n",
    "if 'final_dataset' in locals():\n",
    "    results = execute_complete_pipeline()\n",
    "else:\n",
    "    print(\"Error: final_dataset not found. Run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca83bed",
   "metadata": {},
   "source": [
    "### 2.7.9 Dataset Information Extraction and Validation\n",
    "*Extract and validate actual dataset characteristics (no hard-coding)*\n",
    "\n",
    "**Validated Information:**\n",
    "- **Image Dimensions**: Dynamic extraction from data\n",
    "- **Class Count**: Based on real detection labels\n",
    "- **Class Names**: Proper mapping (Infected, Healthy, WBC)\n",
    "- **Split Sizes**: Real train/val/test counts\n",
    "- **Parasitemia Range**: Min/max from calculated scores\n",
    "- **Class Distribution**: Real object counts per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b11181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_dataset_info(results):\n",
    "    \n",
    "    if not results or 'train_samples' not in results:\n",
    "        print(\"No results available!\")\n",
    "        return None\n",
    "    \n",
    "    train_samples = results['train_samples']\n",
    "    val_samples = results['val_samples']\n",
    "    test_samples = results['test_samples']\n",
    "    all_samples = train_samples + val_samples + test_samples\n",
    "    \n",
    "    # Get actual image size from first sample\n",
    "    sample_image = train_samples[0]['image']\n",
    "    actual_image_size = list(sample_image.shape)\n",
    "    \n",
    "    # Get actual class names from detection labels\n",
    "    all_labels = []\n",
    "    for sample in train_samples + val_samples + test_samples:\n",
    "        labels = sample['detection']['labels']\n",
    "        all_labels.extend(labels.tolist() if hasattr(labels, 'tolist') else labels)\n",
    "    \n",
    "    unique_labels = sorted(set(all_labels))\n",
    "    actual_num_classes = len(unique_labels)\n",
    "    \n",
    "    # Map labels to actual class names from your cell type mapping\n",
    "    cell_type_mapping = {\n",
    "        'parasitized': 0, 'Parasitized': 0,    # Infected\n",
    "        'uninfected': 1, 'Uninfected': 1,     # Healthy  \n",
    "        'white_blood_cell': 2, 'White_Blood_Cell': 2  # WBC\n",
    "    }\n",
    "    \n",
    "    # Reverse mapping to get class names\n",
    "    label_to_name = {0: 'Infected', 1: 'Healthy', 2: 'WBC'}\n",
    "    actual_class_names = [label_to_name.get(label, f'Class_{label}') for label in unique_labels]\n",
    "    \n",
    "    # Get actual heatmap type\n",
    "    sample_heatmap = train_samples[0]['localization']['heatmap']\n",
    "    actual_heatmap_size = list(sample_heatmap.shape)\n",
    "    \n",
    "    # Get actual parasitemia score range\n",
    "    all_scores = [sample['regression']['parasitemia_score'] for sample in train_samples + val_samples + test_samples]\n",
    "    \n",
    "    # Comprehensive heatmap analysis\n",
    "    print(\"Analyzing heatmap characteristics across all samples...\")\n",
    "    \n",
    "    heatmap_stats = {\n",
    "        'min_values': [],\n",
    "        'max_values': [],\n",
    "        'mean_values': [],\n",
    "        'std_values': [],\n",
    "        'coverage_percentages': [],\n",
    "        'non_zero_pixels': [],\n",
    "        'intensity_ranges': []\n",
    "    }\n",
    "    \n",
    "    # Analyze by infection level with enhanced metrics\n",
    "    infection_level_heatmaps = {\n",
    "        'negative': [],\n",
    "        'low': [],\n",
    "        'moderate': [],\n",
    "        'high': []\n",
    "    }\n",
    "    \n",
    "    for sample in train_samples + val_samples + test_samples:\n",
    "        heatmap = sample['localization']['heatmap']\n",
    "        parasitemia = sample['regression']['parasitemia_score']\n",
    "        \n",
    "        # Overall statistics\n",
    "        heatmap_stats['min_values'].append(float(heatmap.min()))\n",
    "        heatmap_stats['max_values'].append(float(heatmap.max()))\n",
    "        heatmap_stats['mean_values'].append(float(heatmap.mean()))\n",
    "        heatmap_stats['std_values'].append(float(heatmap.std()))\n",
    "        \n",
    "        # Coverage analysis\n",
    "        non_zero_pixels = np.sum(heatmap > 0.01)  # Threshold for actual infection\n",
    "        total_pixels = heatmap.size\n",
    "        coverage = (non_zero_pixels / total_pixels) * 100\n",
    "        \n",
    "        heatmap_stats['coverage_percentages'].append(coverage)\n",
    "        heatmap_stats['non_zero_pixels'].append(int(non_zero_pixels))\n",
    "        heatmap_stats['intensity_ranges'].append(float(heatmap.max() - heatmap.min()))\n",
    "        \n",
    "        # Categorize by infection level with enhanced metrics\n",
    "        heatmap_data = {\n",
    "            'min_intensity': float(heatmap.min()),\n",
    "            'max_intensity': float(heatmap.max()),\n",
    "            'mean_intensity': float(heatmap.mean()),\n",
    "            'std_intensity': float(heatmap.std()),\n",
    "            'coverage': coverage,\n",
    "            'non_zero_pixels': int(non_zero_pixels),\n",
    "            'intensity_range': float(heatmap.max() - heatmap.min())\n",
    "        }\n",
    "        \n",
    "        if parasitemia == 0:\n",
    "            infection_level_heatmaps['negative'].append(heatmap_data)\n",
    "        elif parasitemia <= 2:\n",
    "            infection_level_heatmaps['low'].append(heatmap_data)\n",
    "        elif parasitemia <= 10:\n",
    "            infection_level_heatmaps['moderate'].append(heatmap_data)\n",
    "        else:\n",
    "            infection_level_heatmaps['high'].append(heatmap_data)\n",
    "    \n",
    "    # Calculate comprehensive summary statistics\n",
    "    heatmap_summary = {\n",
    "        'overall': {\n",
    "            'min_range': [min(heatmap_stats['min_values']), max(heatmap_stats['min_values'])],\n",
    "            'max_range': [min(heatmap_stats['max_values']), max(heatmap_stats['max_values'])],\n",
    "            'avg_min': np.mean(heatmap_stats['min_values']),\n",
    "            'avg_max': np.mean(heatmap_stats['max_values']),\n",
    "            'avg_mean': np.mean(heatmap_stats['mean_values']),\n",
    "            'avg_std': np.mean(heatmap_stats['std_values']),\n",
    "            'avg_coverage': np.mean(heatmap_stats['coverage_percentages']),\n",
    "            'avg_non_zero_pixels': np.mean(heatmap_stats['non_zero_pixels']),\n",
    "            'intensity_range': [min(heatmap_stats['intensity_ranges']), max(heatmap_stats['intensity_ranges'])],\n",
    "            'avg_intensity_range': np.mean(heatmap_stats['intensity_ranges'])\n",
    "        },\n",
    "        'by_infection_level': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate detailed stats by infection level\n",
    "    for level, heatmaps in infection_level_heatmaps.items():\n",
    "        if heatmaps:\n",
    "            min_intensities = [h['min_intensity'] for h in heatmaps]\n",
    "            max_intensities = [h['max_intensity'] for h in heatmaps]\n",
    "            mean_intensities = [h['mean_intensity'] for h in heatmaps]\n",
    "            std_intensities = [h['std_intensity'] for h in heatmaps]\n",
    "            coverages = [h['coverage'] for h in heatmaps]\n",
    "            non_zero_pixels = [h['non_zero_pixels'] for h in heatmaps]\n",
    "            intensity_ranges = [h['intensity_range'] for h in heatmaps]\n",
    "            \n",
    "            heatmap_summary['by_infection_level'][level] = {\n",
    "                'count': len(heatmaps),\n",
    "                'avg_min_intensity': np.mean(min_intensities),\n",
    "                'min_intensity_range': [min(min_intensities), max(min_intensities)],\n",
    "                'avg_max_intensity': np.mean(max_intensities),\n",
    "                'max_intensity_range': [min(max_intensities), max(max_intensities)],\n",
    "                'avg_mean_intensity': np.mean(mean_intensities),\n",
    "                'mean_intensity_range': [min(mean_intensities), max(mean_intensities)],\n",
    "                'avg_std_intensity': np.mean(std_intensities),\n",
    "                'std_intensity_range': [min(std_intensities), max(std_intensities)],\n",
    "                'avg_coverage': np.mean(coverages),\n",
    "                'coverage_range': [min(coverages), max(coverages)],\n",
    "                'avg_non_zero_pixels': np.mean(non_zero_pixels),\n",
    "                'non_zero_pixels_range': [min(non_zero_pixels), max(non_zero_pixels)],\n",
    "                'avg_intensity_range': np.mean(intensity_ranges),\n",
    "                'intensity_range_span': [min(intensity_ranges), max(intensity_ranges)]\n",
    "            }\n",
    "        else:\n",
    "            heatmap_summary['by_infection_level'][level] = {\n",
    "                'count': 0,\n",
    "                'avg_min_intensity': 0.0,\n",
    "                'min_intensity_range': [0.0, 0.0],\n",
    "                'avg_max_intensity': 0.0,\n",
    "                'max_intensity_range': [0.0, 0.0],\n",
    "                'avg_mean_intensity': 0.0,\n",
    "                'mean_intensity_range': [0.0, 0.0],\n",
    "                'avg_std_intensity': 0.0,\n",
    "                'std_intensity_range': [0.0, 0.0],\n",
    "                'avg_coverage': 0.0,\n",
    "                'coverage_range': [0.0, 0.0],\n",
    "                'avg_non_zero_pixels': 0.0,\n",
    "                'non_zero_pixels_range': [0.0, 0.0],\n",
    "                'avg_intensity_range': 0.0,\n",
    "                'intensity_range_span': [0.0, 0.0]\n",
    "            }\n",
    "    \n",
    "    # Get actual split sizes\n",
    "    actual_splits = {\n",
    "        'train_samples': len(train_samples),\n",
    "        'val_samples': len(val_samples), \n",
    "        'test_samples': len(test_samples),\n",
    "        'total_samples': len(train_samples) + len(val_samples) + len(test_samples)\n",
    "    }\n",
    "    \n",
    "    # Count actual objects per class\n",
    "    class_counts = {label: 0 for label in unique_labels}\n",
    "    for sample in train_samples + val_samples + test_samples:\n",
    "        labels = sample['detection']['labels']\n",
    "        for label in (labels.tolist() if hasattr(labels, 'tolist') else labels):\n",
    "            class_counts[label] += 1\n",
    "            \n",
    "    # Mask details (dynamic, no hard code)\n",
    "    mask_shapes = [list(sample['segmentation']['mask'].shape) for sample in all_samples if 'segmentation' in sample]\n",
    "    mask_paths = [f\"masks/{sample['image_id']}.npy\" for sample in all_samples if 'segmentation' in sample]\n",
    "\n",
    "    # Severity details (dynamic, no hard code)\n",
    "    severity_classes = sorted(set(sample['severity']['severity_class'] for sample in all_samples if 'severity' in sample))\n",
    "    severity_labels = sorted(set(sample['severity']['severity_label'] for sample in all_samples if 'severity' in sample))\n",
    "    \n",
    "    actual_info = {\n",
    "        'total_samples': actual_splits['total_samples'],\n",
    "        'image_size': actual_image_size,\n",
    "        'num_classes': actual_num_classes,\n",
    "        'class_names': actual_class_names,\n",
    "        'class_labels': {name: idx for idx, name in enumerate(actual_class_names)},\n",
    "        'heatmap_size': actual_heatmap_size,\n",
    "        'heatmap_type': 'infection_only',\n",
    "        'heatmap_analysis': heatmap_summary,\n",
    "        'splits': actual_splits,\n",
    "        'parasitemia_range': [min(all_scores), max(all_scores)],\n",
    "        'class_distribution': {actual_class_names[i]: class_counts.get(i, 0) for i in unique_labels},\n",
    "        'mask_shapes': mask_shapes,\n",
    "        'mask_paths': mask_paths,\n",
    "        'severity_classes': severity_classes,\n",
    "        'severity_labels': severity_labels,\n",
    "        'supported_tasks': ['detection', 'regression', 'localization', 'segmentation', 'severity', 'multi_task']\n",
    "    }\n",
    "    \n",
    "    return actual_info\n",
    "\n",
    "def print_actual_info(info):\n",
    "    \"\"\"Print the actual dataset info with pandas table presentation\"\"\"\n",
    "    if not info:\n",
    "        return\n",
    "        \n",
    "    print(\"ACTUAL DATASET INFO:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Samples: {info['total_samples']}\")\n",
    "    print(f\"Image Size: {info['image_size']}\")\n",
    "    print(f\"Number of Classes: {info['num_classes']}\")\n",
    "    print(f\"Class Names: {info['class_names']}\")\n",
    "    print(f\"Class Labels: {info['class_labels']}\")\n",
    "    print(f\"Heatmap Size: {info['heatmap_size']}\")\n",
    "    print(f\"Heatmap Type: {info['heatmap_type']}\")\n",
    "    print(f\"Splits: {info['splits']}\")\n",
    "    print(f\"Parasitemia Range: {info['parasitemia_range']}\")\n",
    "    print(f\"Class Distribution: {info['class_distribution']}\")\n",
    "    print(f\"Mask Shapes: {max(info.get('mask_shapes', 'N/A'))}\")\n",
    "    print(f\"Mask Paths: {info.get('mask_paths', 'N/A')[:1]}\")\n",
    "    print(f\"Severity Classes: {info.get('severity_classes', 'N/A')}\")\n",
    "    print(f\"Severity Labels: {info.get('severity_labels', 'N/A')}\")\n",
    "    \n",
    "    \n",
    "    print(f\"Supported Tasks: {info['supported_tasks']}\")\n",
    "    \n",
    "    # Pandas table presentation\n",
    "    if 'heatmap_analysis' in info:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"HEATMAP ANALYSIS - COMPREHENSIVE STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        overall = info['heatmap_analysis']['overall']\n",
    "        \n",
    "        # Overall Statistics Table\n",
    "        print(f\"\\nOVERALL HEATMAP STATISTICS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        overall_data = {\n",
    "            'Metric': ['Average Min Intensity', 'Average Max Intensity', 'Average Mean Intensity', \n",
    "                      'Average Std Intensity', 'Average Coverage (%)', 'Avg Non-Zero Pixels',\n",
    "                      'Min Value Range', 'Max Value Range', 'Avg Intensity Range'],\n",
    "            'Value': [f\"{overall['avg_min']:.3f}\", f\"{overall['avg_max']:.3f}\", f\"{overall['avg_mean']:.3f}\",\n",
    "                     f\"{overall['avg_std']:.3f}\", f\"{overall['avg_coverage']:.1f}%\", f\"{overall['avg_non_zero_pixels']:.0f}\",\n",
    "                     f\"{overall['min_range'][0]:.3f} - {overall['min_range'][1]:.3f}\",\n",
    "                     f\"{overall['max_range'][0]:.3f} - {overall['max_range'][1]:.3f}\",\n",
    "                     f\"{overall['avg_intensity_range']:.3f}\"],\n",
    "            'Description': ['Baseline infection intensity', 'Peak infection intensity', 'Overall infection density',\n",
    "                           'Intensity variation', 'Infected area coverage', 'Infection pixel count',\n",
    "                           'Global minimum bounds', 'Global maximum bounds', 'Average intensity spread']\n",
    "        }\n",
    "        \n",
    "        overall_df = pd.DataFrame(overall_data)\n",
    "        print(overall_df.to_string(index=False, max_colwidth=25))\n",
    "        \n",
    "        # Infection Level Statistics Table\n",
    "        print(f\"\\nHEATMAP STATISTICS BY INFECTION LEVEL:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        by_level = info['heatmap_analysis']['by_infection_level']\n",
    "        \n",
    "        # Create comprehensive comparison table\n",
    "        level_data = []\n",
    "        for level, stats in by_level.items():\n",
    "            if stats['count'] > 0:\n",
    "                level_data.append({\n",
    "                    'Infection Level': level.upper(),\n",
    "                    'Samples': stats['count'],\n",
    "                    'Avg Min Intensity': f\"{stats['avg_min_intensity']:.3f}\",\n",
    "                    'Min Range': f\"{stats['min_intensity_range'][0]:.3f}-{stats['min_intensity_range'][1]:.3f}\",\n",
    "                    'Avg Max Intensity': f\"{stats['avg_max_intensity']:.3f}\",\n",
    "                    'Max Range': f\"{stats['max_intensity_range'][0]:.3f}-{stats['max_intensity_range'][1]:.3f}\",\n",
    "                    'Avg Mean Intensity': f\"{stats['avg_mean_intensity']:.3f}\",\n",
    "                    'Mean Range': f\"{stats['mean_intensity_range'][0]:.3f}-{stats['mean_intensity_range'][1]:.3f}\",\n",
    "                    'Avg Coverage (%)': f\"{stats['avg_coverage']:.1f}%\",\n",
    "                    'Coverage Range': f\"{stats['coverage_range'][0]:.1f}%-{stats['coverage_range'][1]:.1f}%\",\n",
    "                    'Avg Non-Zero Pixels': f\"{stats['avg_non_zero_pixels']:.0f}\",\n",
    "                    'Pixels Range': f\"{stats['non_zero_pixels_range'][0]:.0f}-{stats['non_zero_pixels_range'][1]:.0f}\"\n",
    "                })\n",
    "            else:\n",
    "                level_data.append({\n",
    "                    'Infection Level': level.upper(),\n",
    "                    'Samples': 0,\n",
    "                    'Avg Min Intensity': 'N/A',\n",
    "                    'Min Range': 'N/A',\n",
    "                    'Avg Max Intensity': 'N/A', \n",
    "                    'Max Range': 'N/A',\n",
    "                    'Avg Mean Intensity': 'N/A',\n",
    "                    'Mean Range': 'N/A',\n",
    "                    'Avg Coverage (%)': 'N/A',\n",
    "                    'Coverage Range': 'N/A',\n",
    "                    'Avg Non-Zero Pixels': 'N/A',\n",
    "                    'Pixels Range': 'N/A'\n",
    "                })\n",
    "        \n",
    "        level_df = pd.DataFrame(level_data)\n",
    "        \n",
    "        # Display in sections for better readability\n",
    "        print(\"\\nINTENSITY METRICS:\")\n",
    "        intensity_cols = ['Infection Level', 'Samples', 'Avg Min Intensity', 'Min Range', \n",
    "                         'Avg Max Intensity', 'Max Range', 'Avg Mean Intensity', 'Mean Range']\n",
    "        print(level_df[intensity_cols].to_string(index=False, max_colwidth=15))\n",
    "        \n",
    "        print(\"\\nCOVERAGE METRICS:\")\n",
    "        coverage_cols = ['Infection Level', 'Samples', 'Avg Coverage (%)', 'Coverage Range', \n",
    "                        'Avg Non-Zero Pixels', 'Pixels Range']\n",
    "        print(level_df[coverage_cols].to_string(index=False, max_colwidth=15))\n",
    "        \n",
    "        # Summary Analysis\n",
    "        print(f\"\\nANALYSIS SUMMARY:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Find most and least infected levels\n",
    "        active_levels = [(level, stats) for level, stats in by_level.items() if stats['count'] > 0]\n",
    "        if active_levels:\n",
    "            max_coverage_level = max(active_levels, key=lambda x: x[1]['avg_coverage'])\n",
    "            max_intensity_level = max(active_levels, key=lambda x: x[1]['avg_max_intensity'])\n",
    "            \n",
    "            print(f\"Highest Coverage:     {max_coverage_level[0].upper()} ({max_coverage_level[1]['avg_coverage']:.1f}%)\")\n",
    "            print(f\"Highest Intensity:    {max_intensity_level[0].upper()} ({max_intensity_level[1]['avg_max_intensity']:.3f})\")\n",
    "            print(f\"Total Active Levels:  {len(active_levels)}/4\")\n",
    "            print(f\"Total Samples:        {sum(stats['count'] for _, stats in active_levels)}\")\n",
    "            \n",
    "            # Quality assessment\n",
    "            overall_quality = \"EXCELLENT\" if overall['avg_max'] > 0.8 else \"GOOD\" if overall['avg_max'] > 0.6 else \"MODERATE\"\n",
    "            coverage_quality = \"OPTIMAL\" if 5 <= overall['avg_coverage'] <= 15 else \"ACCEPTABLE\" if overall['avg_coverage'] < 25 else \"HIGH\"\n",
    "            \n",
    "            print(f\"Intensity Quality:    {overall_quality} (avg max: {overall['avg_max']:.3f})\")\n",
    "            print(f\"Coverage Quality:     {coverage_quality} ({overall['avg_coverage']:.1f}%)\")\n",
    "\n",
    "\n",
    "# Get the actual info with comprehensive heatmap analysis\n",
    "if 'results' in locals():\n",
    "    actual_dataset_info = get_actual_dataset_info(results)\n",
    "    print_actual_info(actual_dataset_info)\n",
    "else:\n",
    "    print(\"Run the pipeline first to get results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab63b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
