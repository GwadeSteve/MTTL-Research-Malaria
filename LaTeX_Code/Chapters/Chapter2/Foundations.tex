\chapter{Theoretical Foundations and Literature Review}
\label{chap:theory}
Deep Learning (\ac{DL}), a branch of Machine Learning (\ac{ML}), has revolutionized how machines analyze complex data, especially visual information like medical images \parencite{Litjens2017SurveyDeepLearningMedical}. Its fundamental strength, as highlighted by reviews such as \textcite{alzubaidi2021review}, is the ability to learn representations of features directly from raw data, obviating the need for manual, domain-specific feature engineering. This chapter covers the foundational principles of \ac{DL} and Convolutional Neural Networks (\acp{CNN}). We will define their core components and mathematical foundations, describe the learning process, and discuss the principal challenges encountered when applying these models to medical data. This foundation is essential to understand why advanced learning paradigms like Transfer and Multitask Learning are not merely advantageous, but often necessary.

\section{Deep Neural Networks as Composite Functions}
\label{sec:deep_network_composition}

\ac{DL} models are a class of \acp{ANN} distinguished by their depth, typically referring to a significant number of layers. Mathematically, a deep network $h(x; \theta)$ is a composite function that maps an input $x$ to an output through a series of $L$ nested transformations.\\

\noindent
This process can be defined iteratively. Let the input be the zeroth-layer activation, $A^{(0)} = x$. The activation $A^{(i)}$ of the $i$-th layer is computed by applying a transformation $h^{(i)}$ (parameterized by $\theta_i$) to the activation of the preceding layer:
\begin{equation}
	A^{(i)} = h^{(i)}(A^{(i-1)}; \theta_i) \quad \text{for } i = 1, \dots, L
	\myequations{Iterative Layer Transformation}
	\label{eq:dnn_layer_transform}
\end{equation}

\noindent
The final output of the network is the activation of the last layer, $y = A^{(L)}$. This unrolled, iterative definition results in the fully composite function:
\begin{equation}
	h(x; \theta) = h^{(L)}(h^{(L-1)}(\dots h^{(1)}(x; \theta_1) \dots; \theta_{L-1}); \theta_L)
	\myequations{Deep Network Composite Function}
	\label{eq:dnn_forward_pass}
\end{equation}

\noindent
The complete set of model parameters, $\theta = \{\theta_1, \dots, \theta_L\}$, is learned from data via an optimization process. The hierarchical, multi-layered structure enables the model to discover and represent intricate structures within the data at multiple, increasing levels of abstraction.

\subsection{Hierarchical Representations}
\label{ssec:dl_learned_features_power}
Traditional \ac{ML} workflows depended on handcrafted features (e.g., SIFT \parencite{Lowe2004DistinctiveImageFeatures}, \ac{HOG} \parencite{Dalal2005HistogramsOrientedGradients}, \ac{LBP} \parencite{Ojala2002MultiresolutionGrayScaleRotation}), which required significant domain expertise and were often brittle. \ac{DL} models, and \acp{CNN} in particular, automate this process of feature discovery \parencite{LeCun2015DeepLearning}. The hierarchical composition of functions enables a layer-wise abstraction of features:
\begin{itemize}
	\item \textbf{Early Layers:} Learn to detect primitive features such as edges, corners, and color gradients.
	\item \textbf{Intermediate Layers:} Combine these primitives into more complex motifs, textures, and parts of objects.
	\item \textbf{Later Layers:} Assemble motifs into high-level, task
	relevant representations of entire objects or concepts.
\end{itemize}
Automated learning of a feature hierarchy is the primary advantage of deep learning models, especially in medical imaging where the relevant visual patterns may be too subtle for human-engineered features to capture effectively.

\input{Chapters/Chapter2/hierarchical_diagram.tex} 

\subsection{Convolutional Neural Networks}
\label{ssec:dl_cnns_visual_specialists}
\acp{CNN} are a specialized class of neural networks designed for processing grid-like data, such as images \parencite{LeCun2015DeepLearning}. Their architecture is inspired by the human visual cortex and incorporates strong \textbf{inductive biases} that make them exceptionally effective for visual tasks:
\begin{itemize}
	\item \textbf{Local Receptive Fields:} Neurons are connected only to small, local regions of the input, exploiting the spatial locality of features in images.
	\item \textbf{Parameter Sharing:} The same set of weights (a filter or kernel) is applied across all spatial locations of the input. This drastically reduces the number of learnable parameters and builds in \textbf{translation equi-variance} the ability to detect a feature regardless of where it appears in the image.
	\item \textbf{Spatial Sub-sampling:} Pooling layers progressively reduce the spatial dimensions of the data, creating representations that are more robust to small shifts and distortions.
\end{itemize}
Notable \ac{CNN} architectures that build upon these principles include LeNet-5 \parencite{LeCun1998GradientBasedLearning}, AlexNet \parencite{Krizhevsky2012ImageNetClassification}, VGGNet \parencite{Simonyan2014VeryDeepConvolutional}, GoogLeNet \parencite{Szegedy2015GoingDeeperConvolutions}, ResNet \parencite{He2016DeepResidualLearning}, and U-Net \parencite{RonnebergerFB15}. More recently, Vision Transformers (\acp{ViT}) \parencite{Dosovitskiy2020ImageWorth16x16Words} have emerged as a powerful alternative, which uses attention mechanisms instead of convolutions.
\begin{figure}[htbp]
	\centering
	
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth, height=4cm, keepaspectratio]{Images/LeNet5.png}
		\caption{LeNet-5 \parencite{LeCun1998GradientBasedLearning}}
		\label{fig:lenet_arch}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth} 
		\centering
		\includegraphics[width=\linewidth, height=4cm, keepaspectratio]{Images/AlexNet-Architecture.png}
		\caption{AlexNet \parencite{Krizhevsky2012ImageNetClassification}}
		\label{fig:alexnet_arch}
	\end{subfigure}
	
	\vspace{1cm}
	
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth, height=4cm, keepaspectratio]{Images/ResidualNet.jpeg}
		\caption{ResNet (Residual Block) \parencite{He2016DeepResidualLearning}}
		\label{fig:resnet_block_arch}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth, height=4cm, keepaspectratio]{Images/The-3D-Unet-model.png}
		\caption{U-Net \parencite{RonnebergerFB15}}
		\label{fig:unet_arch}
	\end{subfigure}
	
	\caption{Some notable \ac{CNN} architectures.}
	\label{fig:cnn_architectures_gallery}
\end{figure}

\section{CNN Architecture Components}
\label{sec:dl_core_cnn_components}
A modern \ac{CNN} is constructed by composing a sequence of fundamental layers.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[
		scale=0.85, every node/.style={scale=0.85, font=\footnotesize, align=center},
		data_flow/.style={rectangle, draw, minimum height=1.2cm, minimum width=1.2cm, fill=gray!10, inner sep=2pt},
		op_block/.style={rectangle, draw, fill=blue!10, minimum height=0.8cm, text width=1.8cm, rounded corners=1pt, inner sep=3pt},
		fc_block/.style={rectangle, draw, fill=red!10, minimum height=1cm, minimum width=1cm, rounded corners=1pt, inner sep=3pt},
		arrow_style/.style={-Stealth, thick, shorten >=1pt, shorten <=1pt}
		]
		
		\node[data_flow, fill=blue!20, minimum width=1.5cm, label={[font=\tiny]below:Image}] (input_data) {};
		\draw (input_data.north east) -- ++(0.2,0.2) -- ++(0,-1.2*0.85) -- (input_data.south east); 
		\draw (input_data.north east) ++(0.2,0.2) -- ++(-1.5*0.85,0) -- (input_data.north west);
		
		\node[op_block, right=0.7cm of input_data, fill=green!20] (conv1_op) {Conv+ReLU};
		\node[data_flow, right=0.2cm of conv1_op, minimum width=1cm, minimum height=1cm, label={[font=\tiny]below:Maps 1}] (maps1_data) {};
		\draw (maps1_data.north east) -- ++(0.15,0.15) -- ++(0,-1.0*0.85) -- (maps1_data.south east);
		\draw (maps1_data.north east) ++(0.15,0.15) -- ++(-1.0*0.85,0) -- (maps1_data.north west);
		\node[op_block, right=0.2cm of maps1_data, fill=orange!30] (pool1_op) {Pool};
		
		\node[data_flow, right=0.2cm of pool1_op, minimum width=0.7cm, minimum height=0.7cm, label={[font=\tiny]below:Maps 2}] (maps2_data) {};
		\draw (maps2_data.north east) -- ++(0.1,0.1) -- ++(0,-0.7*0.85) -- (maps2_data.south east);
		\draw (maps2_data.north east) ++(0.1,0.1) -- ++(-0.7*0.85,0) -- (maps2_data.north west);
		
		\node[right=0.5cm of maps2_data] (dots) {\dots};
		
		\node[op_block, fill=purple!20, right=0.5cm of dots, text width=1cm] (flatten_op) {Flatten};
		\node[data_flow, right=0.2cm of flatten_op, minimum width=0.3cm, minimum height=1.5cm, label={[font=\tiny]below:Vector}] (vector_data) {}; 
		\node[fc_block, fill=red!20, right=0.5cm of vector_data] (fc_op) {FC};
		\node[data_flow, fill=yellow!30, right=0.2cm of fc_op, minimum width=0.5cm, label={[font=\tiny]below:Output}] (output_data) {};
		
		\draw[arrow_style] (input_data) -- (conv1_op);
		\draw[arrow_style] (conv1_op) -- (maps1_data);
		\draw[arrow_style] (maps1_data) -- (pool1_op);
		\draw[arrow_style] (pool1_op) -- (maps2_data);
		\draw[arrow_style] (maps2_data) -- (dots);
		\draw[arrow_style] (dots) -- (flatten_op);
		\draw[arrow_style] (flatten_op) -- (vector_data);
		\draw[arrow_style] (vector_data) -- (fc_op);
		\draw[arrow_style] (fc_op) -- (output_data);
		
	\end{tikzpicture}
	\caption{Basic building blocks of a \ac{CNN}}
	\label{fig:cnn_building_blocks}
\end{figure}

\subsection*{Convolutional Layers}
\label{ssec:dl_conv_layers_detail}
This is the primary building block of a \ac{CNN}, it works by sliding a small, learnable filter over an input volume. At each position, it computes a dot product between the filter's weights and the corresponding input patch. This process, illustrated in Figure~\ref{fig:convolution_OP}, generates a feature map that highlights specific patterns such as a vertical edge or a particular texture detected by the filter.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[
		scale=0.8, every node/.style={scale=0.8, font=\footnotesize, align=center},
		cell/.style={rectangle, draw, minimum size=0.7cm, inner sep=2pt},
		filter_cell/.style={rectangle, draw=blue, fill=blue!10, minimum size=0.7cm, inner sep=2pt},
		output_cell/.style={rectangle, draw=red, fill=red!10, minimum size=0.7cm, inner sep=2pt},
		op_symbol/.style={circle, draw, fill=yellow!30, minimum size=0.5cm, inner sep=1pt, font=\Large},
		arrow_style/.style={-Stealth, thick, shorten >=1pt, shorten <=1pt}
		]
		
		\matrix (input_patch) [matrix of nodes, nodes={cell}, column sep=-\pgflinewidth, row sep=-\pgflinewidth, anchor=center] at (0,0) {
			\pgfmathrandominteger{\val}{0}{9}\val & \pgfmathrandominteger{\val}{0}{9}\val & \pgfmathrandominteger{\val}{0}{9}\val \\
			\pgfmathrandominteger{\val}{0}{9}\val & \pgfmathrandominteger{\val}{0}{9}\val & \pgfmathrandominteger{\val}{0}{9}\val \\
			\pgfmathrandominteger{\val}{0}{9}\val & \pgfmathrandominteger{\val}{0}{9}\val & \pgfmathrandominteger{\val}{0}{9}\val \\
		};
		\node[above=0.2cm of input_patch] {Input Patch};
		
		\matrix (filter) [matrix of nodes, nodes={filter_cell}, column sep=-\pgflinewidth, row sep=-\pgflinewidth, anchor=center] at (3,1) { 
			\pgfmathrandominteger{\w}{0}{1}\w & \pgfmathrandominteger{\w}{0}{1}\w \\
			\pgfmathrandominteger{\w}{0}{1}\w & \pgfmathrandominteger{\w}{0}{1}\w \\
		};
		\node[above=0.2cm of filter] {Filter (Kernel)};
		
		\begin{scope}[on background layer]
			\node [fit=(input_patch-1-1) (input_patch-2-2), draw=blue, thick, fill=blue!5, inner sep=-1pt, dashed] {};
		\end{scope}
		
		\node[op_symbol] (operation) at (2.5, -1.5) {$\otimes$}; 
		\node[below=0.1cm of operation, text width=2.5cm, font=\tiny] {Element-wise Product + Sum + Bias};
		
		\node[output_cell] (output_value) at (5, -1.5) {Result};
		\node[above=0.2cm of output_value] {Output Element};
		
		\draw[arrow_style] (input_patch-2-2.south) .. controls +(0,-0.5) and +(-0.5,0.5) .. (operation.west);
		\draw[arrow_style] (filter.south) .. controls +(0,-0.5) and +(0.5,0.5) .. (operation.east);
		\draw[arrow_style] (operation.east) -- (output_value.west);
		
		\draw[->, blue!70!black, dashed, shorten >=2pt, shorten <=2pt] (filter.east) ++(0.2,0) -- ++(0.7,0) node[right, font=\tiny, black] {Filter slides};
		
	\end{tikzpicture}
	\caption{Principle of the convolution operation.}
	\label{fig:convolution_OP}
\end{figure}

\subsection*{Pooling Layers}
\label{ssec:dl_pooling_layers_detail}
Pooling layers perform non-linear down-sampling to reduce the spatial dimensions of feature maps. This decreases computational load, reduces the number of parameters, and provides a degree of translation invariance. The two most common forms are:
\begin{itemize}
	\item \textbf{Max Pooling:} Selects the maximum element from a local region $R$:\\
	\hspace*{\fill} $\text{MaxOut}(R) = \max_{(p,q) \in R} (a_{pq})$ \hspace*{\fill}
	
	\item \textbf{Average Pooling:} Computes the average of elements in region $R$:\\
	\hspace*{\fill} $\text{AvgOut}(R) = \frac{1}{|R|} \sum_{(p,q) \in R} a_{pq}$ \hspace*{\fill}
\end{itemize}
Pooling is usually applied with a fixed window size (e.g., 2x2) and stride (e.g., 2).
\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}[
		scale=0.9, every node/.style={scale=0.9, font=\footnotesize, align=center},
		cell/.style={rectangle, draw, minimum size=0.7cm, rounded corners=2pt, inner sep=2pt},
		input_cell/.style={cell, fill=gray!10},
		max_val_cell/.style={cell, fill=yellow!60, font=\bfseries\large},
		output_cell/.style={cell, fill=red!20, font=\bfseries\Large},
		window_box/.style={draw=blue!70, very thick, dashed, rounded corners=2pt},
		arrow_style/.style={-Stealth, thick, blue!70},
		stride_arrow/.style={->, very thick, dashed, gray!70}
		]
		
		\node[above=0.2cm, font=\small] (input_label) at (1,2.5) {Input (4x4)};
		
		\matrix (input_map) [matrix of nodes, nodes={input_cell},
		column sep=-\pgflinewidth, row sep=-\pgflinewidth,
		anchor=center] at (1,1) {
			1 & 2 & 6 & 3 \\
			3 & 5 & 2 & 1 \\
			1 & 2 & 2 & 1 \\
			7 & 3 & 4 & 8 \\
		};
		
		\node at (input_map-2-2) [max_val_cell] {5};
		\node at (input_map-1-3) [max_val_cell] {6};
		\node at (input_map-4-1) [max_val_cell] {7};
		\node at (input_map-4-4) [max_val_cell] {8};
		
		\node[above=0.2cm, font=\small] (output_label) at (5,2.5) {Output (2x2)};
		
		\matrix (output_map) [matrix of nodes, nodes={output_cell},
		column sep=-\pgflinewidth, row sep=-\pgflinewidth,
		anchor=center] at (5,1) {
			5 & 6 \\
			7 & 8 \\
		};
		
		\node [window_box, fit=(input_map-1-1) (input_map-2-2)] (w1) {};
		\node [window_box, fit=(input_map-1-3) (input_map-2-4)] (w2) {};
		\node [window_box, fit=(input_map-3-1) (input_map-4-2)] (w3) {};
		\node [window_box, fit=(input_map-3-3) (input_map-4-4)] (w4) {};
		
		\draw[arrow_style] (w1) -- (output_map-1-1);
		\draw[arrow_style] (w2) -- (output_map-1-2);
		\draw[arrow_style] (w3) -- (output_map-2-1);
		\draw[arrow_style] (w4) -- (output_map-2-2);
		
		\draw[stride_arrow] ([xshift=0.35cm]input_map-1-1.east) -- ([xshift=0.35cm]input_map-1-3.west) node[midway, above, font=\tiny, blue!70] {};
		\draw[stride_arrow] ([yshift=-0.35cm]input_map-1-1.south) -- ([yshift=-0.35cm]input_map-3-1.north) node[midway, left, font=\tiny, blue!70, rotate=90] {};
		
	\end{tikzpicture}
	\caption{Max pooling with 2x2 windows and stride s=2 from input to output.}
	\label{fig:max_pooling}
\end{figure}

\subsection*{Activation Functions}
\label{ssec:dl_activation_functions_table_ultra_minimal_v2}
An activation function $\sigma(\cdot)$ is applied element-wise to the output of a linear operation (like convolution or a fully-connected layer \ref{ssec:dl_fc_output_layers}). It introduces non-linearity, which is critical for the network to learn complex mappings. The choice of activation function significantly impacts model performance and training dynamics. Common choices include the Sigmoid, Hyperbolic Tangent (Tanh), Rectified Linear Unit (\ac{ReLU}), and Leaky \ac{ReLU} functions, each with distinct properties.

\subsection*{Fully Connected Layers and Output Layers}
\label{ssec:dl_fc_output_layers}
After several stages of convolution and pooling, the high-level feature maps are flattened into a vector and passed to one or more \ac{FC} layers. In a \ac{FC} layer, every input neuron is connected to every output neuron. The final \ac{FC} layer produces the model's output logits, $z$. A final activation function transforms these logits into a desired output format, such as class probabilities using the Softmax function for multi-class classification:
\[ P(y=j|x) = \text{Softmax}(z_j) = \frac{e^{z_j}}{\sum_{k=1}^{C} e^{z_k}} \]
Here, $z_j$ is the logit for class $j$ out of $C$ total classes.

\subsection*{Supervised Learning in CNNs}
\label{sec:dl_supervised_learning_in_cnns}
\acp{CNN} are most commonly trained through supervised learning. Given a dataset of labeled examples $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$, where $x_i$ is an input image and $y_i$ is its corresponding label, the goal is to learn the network parameters $\theta$ that best map inputs to outputs.\\

\noindent The training process is usually framed as an optimization problem where the objective is to find the set of parameters $\theta^*$ that minimizes the \textbf{empirical risk}, the average loss over the training dataset, often with an added regularization term $R(\theta)$:
\[ \theta^* = \arg\min_{\theta} \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(f(x_i; \theta), y_i) + \lambda R(\theta) \]
Here, $\hat{y}_i=f(x_i; \theta)$ is the model's prediction for input $x_i$, and the \textbf{loss function} $\mathcal{L}(\cdot, \cdot)$ quantifies the difference between the prediction $\hat{y}_i$ and the true label $y_i$. Common loss functions include:
\begin{itemize}
	\item \textbf{Cross-Entropy Loss:} For classification tasks. Binary Cross-Entropy (\ac{BCE}) for two-class problems:
	\[ \mathcal{L}_{BCE} = - [y \log(\hat{y}) + (1-y) \log(1-\hat{y})] \]
	\item \textbf{Mean Squared Error (MSE):} For regression tasks:
	\[ \mathcal{L}_{MSE} = \frac{1}{2}(\hat{y} - y)^2 \]
	\item \textbf{Dice Loss:} Commonly used for segmentation tasks to measure overlap between predicted ($P$) and ground truth ($G$) masks: 
	\[ \mathcal{L}_{Dice} = 1 - \frac{2 |P \cap G|}{|P| + |G|} \]
\end{itemize}

\section{Limitations of Training CNNs from Scratch}
\label{sec:dl_limitations_from_scratch}

Although Convolutional Neural Networks (\acp{CNN}) have shown remarkable success, training them from random initialization poses significant challenges, especially in medical imaging.

\subsection*{Data Scarcity}
\label{ssec:dl_data_dependency_scarcity_issue}
Deep models are \textbf{data-hungry}, requiring large, diverse annotated datasets. In medical domains, acquiring such data is often impractical due to costly, time-intensive expert labeling, privacy regulations (e.g., HIPAA, GDPR), and the rarity of certain conditions, which also induces severe class imbalance. Consequently, many medical datasets are orders of magnitude smaller than standard computer vision benchmarks, making training from scratch unreliable.

\subsection*{Overfitting and Limited Generalization}
\label{ssec:dl_overfitting_generalization_problem}
High-capacity models trained on small datasets are prone to \textbf{overfitting}, memorizing training samples rather than learning generalizable patterns. Techniques like regularization (L1/L2 \parencite{Hoerl1970Ridge,Tibshirani1996Lasso}), Dropout \parencite{Srivastava2014Dropout}, and Batch Normalization \parencite{Ioffe2015BatchNormalization} help but cannot fully compensate for scarce data.

\subsection*{Domain Shift}
\label{ssec:dl_domain_shift_sensitivity_issue}
Supervised learning assumes $P_{train}(X,Y) = P_{test}(X,Y)$. In clinical practice, this rarely holds (\textbf{domain shift} \parencite{Kouw2019ReviewDomainAdaptation}) due to differences in imaging devices, acquisition protocols, sample preparation, and patient populations. Domain sensitivity limits the deployability of models trained purely on local datasets.

\subsection*{Class Imbalance}
\label{ssec:dl_class_imbalance_challenge}
Medical datasets often exhibit extreme imbalance, where rare but clinically critical cases are vastly outnumbered. Standard losses bias training toward majority classes, degrading minority-class performance. Remedies include weighted losses (e.g., Focal Loss \parencite{Lin2017Focal}), resampling (e.g., SMOTE \parencite{Chawla2002SMOTE}), and robust metrics (e.g., \acp{AUC}, \ac{ROC} \parencite{Fawcett2006ROC}).

\noindent These limitations scarce data, overfitting, domain shift, and class imbalance make training CNNs from scratch impractical in real-world medical applications like malaria detection. This motivates advanced paradigms designed to enhance robustness and generalization, notably:

\begin{itemize}
	\item \textbf{Transfer Learning (\ac{TL}):} Leveraging knowledge from large-scale datasets to improve performance with limited labeled data (Chapter~\ref{sec:tl_leveraging_knowledge}).
	\item \textbf{Multitask Learning (\ac{MTL}):} Learning related tasks simultaneously to improve efficiency and generalization (Chapter~\ref{sec:mtl_simultaneous_tasks}).
\end{itemize}

\section{Transfer Learning Paradigm}
\label{sec:tl_leveraging_knowledge}
\acf{TL} is a paradigm where knowledge learned in a source domain $(D_S, T_S)$ is transferred to a target domain $(D_T, T_T)$ to improve performance on a target task. This approach is particularly useful in medical imaging where large annotated datasets are difficult to obtain due to privacy concerns and the high cost of expert labeling. General benchmarks like ImageNet \parencite{Deng2009ImageNetAL} provide millions of examples compared to medical data which is often limited. TL allows models to perform effectively in these low resource settings by reusing pre-learned features rather than training from scratch.

\subsection{Definition and Typology}
\label{ssec:tl_def_typo}
The most widely cited formal definition of \ac{TL} is provided by \textcite{pan2009survey}, who defined it in terms of a \textbf{domain} and a \textbf{task}:  
\begin{itemize}
	\item A \textbf{domain} $D$ consists of a feature space $\mathcal{X}$ and a marginal probability distribution $P(X)$, where $X = \{x_1, \dots, x_n\} \in \mathcal{X}$.
	\item A \textbf{task} $T$ is composed of a label space $\mathcal{Y}$ and a predictive function $f(\cdot)$ learned from the training data.
\end{itemize}

\noindent Given a source domain $D_S$ with task $T_S$, and a target domain $D_T$ with task $T_T$, the goal of \textbf{transfer learning} is to improve the learning of the target predictive function $f_T(\cdot)$ in $D_T$ using knowledge from $D_S$ and $T_S$, under the condition that $D_S \neq D_T$ or $T_S \neq T_T$.\\ 

\subsubsection*{Typology of Transfer Learning}
\ac{TL} can be categorized along several complementary dimensions. Zhuang et al. \parencite{zhuang2020comprehensive} proposed a more structured and modern classification of transfer learning methods. They categorized these methods along several complementary dimensions, reflecting the evolution of the field, especially with the rise of \ac{DL}:  

\begin{itemize}
	\item \textbf{By label availability between source and target:}  
	\begin{itemize}
		\item \textit{Inductive TL:} Target domain has labeled data. The source may or may not be labeled. 
		\item \textit{Transductive TL:} Target domain is unlabeled, but labeled source data is available. 
		\item \textit{Unsupervised TL:} Neither source nor target domains contain labels, and transfer relies on unsupervised representation learning.
	\end{itemize}
	
	\item \textbf{By feature space consistency:}  
	\begin{itemize}
		\item \textit{Homogeneous TL:} Source and target share the same feature space (e.g., RGB pixel space), though distributions differ. This is the most common case in medical imaging.  
		\item \textit{Heterogeneous TL:} Feature spaces differ (e.g., transferring between 2D fundus photography and 3D OCT scans). Specialized methods are required to align the modalities.  
	\end{itemize}
	
	\item \textbf{By what is transferred:}  
	\begin{itemize}
		\item \textit{Instance-based TL:} Source samples are reweighted or reused directly in the target training process.  
		\item \textit{Feature-based TL:} Shared or mapped feature representations are constructed to connect domains (e.g., domain-invariant embeddings).  
		\item \textit{Parameter-based TL:} Source trained parameters are reused as initialization or partially shared (e.g., fine-tuning CNN backbones).  
		\item \textit{Relational-based TL:} Structural knowledge such as inter-class relations or graph structures is transferred. 
	\end{itemize}
\end{itemize}

\subsection{Common Transfer Learning Techniques in Deep Learning}
\label{ssec:tl_common_techniques_revised} 
Several practical techniques facilitate parameter-based and feature-based \ac{TL} within \ac{DL}. 
\subsubsection*{Fine-tuning Pre-trained Models} 
\label{sssec:tl_fine_tuning_pre_trained_methods} Fine-tuning is the most common parameter-based \ac{TL} technique. It adapts a model pre-trained on a large source dataset like \textcite{Deng2009ImageNetAL} for a target task by continuing the training process on the target data. The primary strategies, shown in Figure~\ref{fig:tl_finetuning_strategies_legend_minimal} are:
\begin{itemize} 
	\item \textbf{Full Fine-tuning:} All layers of the pre-trained network are updated using a small learning rate. This is suitable when the target dataset is sufficiently large and similar to the source. 
	\item \textbf{Feature Extraction (Frozen Layers):} The convolutional base of the pre-trained model is frozen, acting as a fixed feature extractor. Only a new, task-specific classifier head is trained. This is ideal for very small target datasets to prevent overfitting. \item \textbf{Partial Fine-tuning:} A hybrid approach where early layers (capturing generic features like edges) are frozen, while later layers (capturing more abstract features) are fine-tuned along with the new head. This balances feature reuse and task-specific adaptation. 
\end{itemize}

\begin{figure}[htbp]
	\centering
	\resizebox{0.8\textwidth}{!}{
		\input{Chapters/Chapter2/finetuning_overview.tex}
	}
	\caption{Conceptual overview of three common fine-tuning strategies.}
	\label{fig:tl_finetuning_strategies_legend_minimal}
\end{figure}

\subsubsection*{Learning Rate Management}
\label{sssec:tl_learning_rate_management}
Effective fine-tuning requires careful learning rate management to avoid catastrophic forgetting of valuable pre-trained features. A common practice is using \textbf{differential learning rates}, where different parts of the network have different update speeds. Let $\theta = \{\theta_1, \dots, \theta_L\}$ be the parameters of the model's layers. The update rule for layer $i$ becomes:
\[ \theta_i \leftarrow \theta_i - \eta_i \nabla_{\theta_i} \mathcal{L} \]
where $\eta_i$ is the learning rate for the $i$-th layer. Typically, earlier layers are assigned a smaller learning rate than later layers ($\eta_i < \eta_j$ for $i < j$), allowing the model to preserve general features while adapting task-specific ones more aggressively.

\subsubsection*{\acf{PEFT}}
\label{sssec:tl_adapter_modules_efficiency}
\acf{PEFT} adapts large pretrained models by training only a small set of extra parameters while freezing the majority, enabling efficient transfer with minimal storage and compute \parencite{Houlsby2019ParameterEfficientTransferLearning}. A key example is \textbf{adapter modules}, precisely \textbf{Low-Rank Adaptation (LoRA)} \parencite{Hu2021LoRA} which inserts trainable low-rank matrices into the weight update path. For an input $\mathbf{z}$ (Fig.~\ref{fig:lora_module}):
\begin{equation}
	\mathbf{z}' = \mathbf{z} + \frac{\alpha}{r} W_B W_A \mathbf{z}.
	\myequations{LoRa Update equation.}
	\label{eq:lora_update}
\end{equation}
This keeps parameter growth minimal while maintaining strong performance.

\subsection{General Benefits and Limitations of Transfer Learning}
\label{sec:tl_general_benefits_limitations}
\ac{TL} provides clear advantages for model development, particularly in data-scarce domains, but it also comes with important caveats that must be considered.

\subsubsection*{Key Benefits}
\begin{itemize}
	\item \textbf{Improved Performance with Limited Data:} Using pre-trained models allows achieving high performance on a target task with substantially fewer labeled samples than training from scratch, as the model already captures generalizable feature representations.
	\item \textbf{Faster Convergence:} Fine-tuning starts from a well-initialized weight set, reducing the number of epochs needed to reach optimal performance and speeding up training.
	\item \textbf{Better Generalization:} Features learned from large, diverse source datasets (e.g., ImageNet) often provide robust representations that act as a regularizer, lowering the risk of overfitting on small target datasets.
	\item \textbf{Access to Advanced Architectures:} Transfer learning enables the use of state-of-the-art architectures without the computational cost of training them from scratch.
\end{itemize}

\subsubsection*{Limitations and Challenges}
\begin{itemize}
	\item \textbf{Negative Transfer:} If the source and target domains/tasks are too dissimilar, transferred knowledge can bias the model and degrade performance compared to training from scratch \parencite{wang2019characterizing}.
	\item \textbf{Domain Shift:} Differences in data distribution (e.g., natural images vs. medical microscopy) may limit transfer effectiveness, as source features might be irrelevant or misleading for the target task \parencite{Kouw2019ReviewDomainAdaptation}.
	\item \textbf{Inherited Source Bias:} Any biases present in the source dataset such as demographic or equipment-related can propagate to the target model, reducing robustness to out-of-distribution samples.
	\item \textbf{Fine-tuning Complexity:} Selecting which layers to freeze, choosing learning rates, or deciding on partial vs. full fine-tuning often requires extensive experimentation \parencite{raffel2020exploring}.
\end{itemize}

\noindent Overall, \ac{TL} is a powerful tool for accelerating learning and improving generalization, but it must be applied carefully, with attention to domain alignment and task relevance. 

\section{Multitask Learning Paradigm}
\label{sec:mtl_simultaneous_tasks}
\ac{MTL} is a paradigm in which a single model is trained to perform multiple related tasks simultaneously. The central idea, introduced in early work by \textcite{Caruana1997MTL}, is that learning tasks in parallel with a shared representation allows the model to exploit commonalities across tasks, improving generalization. This joint learning acts as a form of inductive transfer, providing regularization that reduces overfitting, particularly in data scarce settings \parencite{Ruder2017OverviewMTL}.

\subsection{Definition and Formulation}
\label{sec:mtl_definition_math}
In an \ac{MTL} setting, we consider $K$ related tasks $\{T_k\}_{k=1}^K$, each with dataset 
$\mathcal{D}_k = \{(x_i^{(k)}, y_i^{(k)})\}_{i=1}^{N_k}$ and loss $\mathcal{L}_k$. 
A model is designed with both shared parameters $\theta_{\text{sh}}$ and task-specific parameters 
$\{\theta_k\}_{k=1}^K$. The overall objective is a weighted sum of task losses:

\begin{equation}
	\mathcal{L}_{\text{MTL}}(\theta_{\text{sh}}, \{\theta_k\}_{k=1}^K) 
	= \sum_{k=1}^{K} \lambda_k \, \mathcal{L}_k \big(f(x; \theta_{\text{sh}}, \theta_k), y_i^{(k)}\big)
	\myequations{Multitask Learning optimization formulation.}
	\label{eq:mtl_loss}
\end{equation}
where $\lambda_k$ balances the contribution of each task. The challenge is to optimize both the parameters and the task weights such that knowledge sharing through $\theta_{sh}$ promotes positive transfer rather than interference.

\subsection{MTL Architectural Approaches}
\label{ssec:mtl_architectural_approaches}

Architectural design is central to how \ac{MTL} takes advantage of shared representations. The two most widely adopted approaches are \textbf{hard parameter sharing} and \textbf{soft parameter sharing}:  

\begin{itemize}
	\item \textbf{Hard Parameter Sharing:} A shared encoder (backbone) captures common features, followed by task-specific heads. This forces the backbone to learn representations useful across all tasks and provides strong regularization.
	\item \textbf{Soft Parameter Sharing:} Each task has its own parameters, but similarity between task models is encouraged through explicit regularization (e.g., penalizing large distances between parameter sets).
\end{itemize}

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.47\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/HardParmSharing.png}
		\caption{Hard parameter sharing in MTL.}
		\label{fig:mtl_hard_sharing}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.47\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/SoftSharing.png}
		\caption{Soft parameter sharing in MTL.}
		\label{fig:mtl_soft_sharing}
	\end{subfigure}
	\caption{Main architectural approaches for MTL.}
	\label{fig:mtl_sharing_comparison}
\end{figure}

More advanced architectures mentioned by \textcite{Ruder2017OverviewMTL}, such as Deep Relationship Networks, Cross-stitch Networks, and Sluice Networks, offer more nuanced mechanisms for controlling knowledge sharing between tasks.

\subsection{Optimization Challenges}
\label{sec:mtl_optimization_challenges}
Unlike \acf{STL}, \ac{MTL} introduces unique training difficulties:

\begin{itemize}
	\item \textbf{Task Balancing:} Uniform task weights ($\lambda_k=1$) often lead to dominance by tasks with larger losses. Dynamic schemes, such as uncertainty weighting \parencite{Kendall2018MultiTaskLearningUncertainty} or GradNorm \parencite{Chen2018GradNorm}, adaptively rescale contributions during training.
	\item \textbf{Negative Transfer and Gradient Conflicts:} Sharing across weakly related tasks may harm performance \parencite{wang2019characterizing}. Conflicting gradients from different tasks can pull shared parameters in opposite directions. Recent methods mitigate this by modifying or projecting gradients to reduce destructive interference \parencite{Yu2020GradientSurgery}.
\end{itemize}

\subsection{General Benefits and Limitations of Multitask Learning}
\label{sec:mtl_general_benefits_limitations}

\subsubsection*{Key Benefits}
\begin{itemize}
	\item \textbf{Implicit Data Augmentation:} Joint training exposes the shared backbone to more varied data distributions, improving representation robustness.
	\item \textbf{Regularization:} Shared parameters constrain the model, reducing overfitting and improving generalization.
	\item \textbf{Feature Relevance:} Auxiliary tasks can guide attention toward informative features that a primary task might underutilize.
	\item \textbf{Efficiency:} A single \ac{MTL} model supports multiple tasks in one forward pass, saving inference cost relative to training separate models.
\end{itemize}

\subsubsection*{Limitations and Challenges}
\begin{itemize}
	\item \textbf{Negative Transfer:} The main risk, arising when tasks are insufficiently related, degrading performance relative to single-task baselines.
	\item \textbf{Task Balancing Complexity:} Optimal weighting of $\lambda_k$ is non-trivial, and poor choices can destabilize training.
	\item \textbf{Architecture Design:} Determining the right degree of sharing between tasks requires careful experimentation.
	\item \textbf{Dependence on Task Relatedness:} Benefits assume relatedness between tasks. If this assumption fails, \ac{MTL} may be counterproductive.
\end{itemize}

\section{Prior Work in Malaria Detection}
\label{sec:prior_work_malaria}
This section reviews prior research in malaria blood smear analysis with a particular emphasis on advanced AI paradigms. We first discuss how transfer learning and related methods have been applied to malaria detection across different imaging modalities. We then examine the emerging but underexplored potential of \acf{MTL} and \acf{MTTL}, which defines our own research focus.

\subsection{Advanced AI Paradigms for Malaria Detection}
\label{ssec:advanced_ai_malaria}
Transfer learning (\ac{TL}) has been the most widely adopted paradigm in malaria detection, largely due to its effectiveness in overcoming limited annotated datasets. Pre-trained convolutional neural networks (CNNs) such as VGG \parencite{Simonyan2014VeryDeepConvolutional}, ResNet \parencite{he2016deep}, DenseNet \parencite{Huang2017DenseNet}, Inception \parencite{Szegedy2015GoogLeNet}, and EfficientNet \parencite{Tan2019EfficientNet} appear frequently in the literature, with ImageNet as the primary source domain \parencite{Deng2009ImageNetAL}. Fine-tuning these models on malaria datasets has consistently yielded high accuracy in the narrow task of binary classification between parasitized and uninfected cells. Table~\ref{tab:tl_studies_summary} summarizes representative works, spanning thin blood smears, thick smears, and smartphone-based microscopy.

\begin{center} 
	\footnotesize 
	\begin{longtable}{@{} 
			>{\raggedright\arraybackslash}p{0.22\textwidth}  % Study (Ref.)
			>{\centering\arraybackslash}p{0.05\textwidth}     % Year 
			>{\raggedright\arraybackslash}p{0.28\textwidth}  % Task
			>{\raggedright\arraybackslash}p{0.20\textwidth}  % Method
			>{\raggedright\arraybackslash}p{0.20\textwidth}@{}}
		\caption{Selected TL Applications in Medical Imaging and Malaria Detection}\label{tab:tl_studies_summary}\\
		\toprule
		\textbf{Study} & \textbf{Year} & \textbf{Task (Dataset/Type)} & \textbf{Method/Model} & \textbf{Key Result/Note} \\ \midrule
		\endfirsthead
		
		\multicolumn{5}{c}{{\bfseries\tablename\ \thetable{}} -- continued from previous page} \\
		\toprule
		\textbf{Study} & \textbf{Year} & \textbf{Task (Dataset/Type)} & \textbf{Method/Model} & \textbf{Key Result/Note} \\ \midrule
		\endhead
		
		\multicolumn{5}{r}{{\textit{Continued on next page}}} \\
		\endfoot
		
		\bottomrule
		\endlastfoot
		
		% --- 2016 ---
		%\multicolumn{5}{l}{\textbf{2016}} \\
		%\addlinespace[0.2em]
		
		%\textcite{Gulshan2016DevelopmentValidationDLAlgorithm} & 2016 & Diabetic Retinopathy Grading (Fundus Images) & InceptionV3 TL & Ophthalmologist-level Acc. \\
		%\addlinespace[0.3em]
		
		% --- 2017 ---
		%\multicolumn{5}{l}{\textbf{2017}} \\
		%\addlinespace[0.2em]
		
		%\textcite{Rajpurkar2017CheXNet} & 2017 & Pneumonia Det. (ChestX-ray14) & DenseNet-121 TL & Radiologist-level Perf. \\
		%\addlinespace[0.3em]
		%\textcite{Esteva2017DermatologistlevelClassificationSkin} & 2017 & Skin Cancer Classif. (Dermoscopy) & InceptionV3 TL & Dermatologist-level Acc. \\
		%\addlinespace[0.3em]
		%\textcite{Bejnordi2017DiagnosticAssessmentDeep} & 2017 & Breast Cancer Metastasis Det. (CAMELYON16 \ac{WSI}) & GoogLeNet TL & High Acc. lymph node \\
		%\addlinespace[0.3em]
		
		% --- 2018 ---
		\multicolumn{5}{l}{\textbf{2018}} \\
		\addlinespace[0.2em]
		
		\rowcolor{gray!15}
		\textcite{Rajaraman2018NIHMalariaDataset} & 2018 & Malaria: Binary Cell Classif. (NIH, Thin Giemsa) & VGG, ResNet, etc. TL & ResNet-50: 95.9\% Acc. \\
		\addlinespace[0.3em]
		
		% --- 2021 ---
		\multicolumn{5}{l}{\textbf{2021}} \\
		\addlinespace[0.2em]
		
		\rowcolor{gray!15}
		\textcite{Abdurahman2021ModifiedYOLO} & 2021 & Malaria: Parasite Det. (Makerere U., Thick N/S) & Modified YOLOv3/v4 & YOLO for thick smears \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{DeepBasedDeconv} & 2021 & Malaria: RBC Det. \& Extraction (NLM Thin Smears) & Modified CFPNet-M (Distance Transform Regression) & 92.2\% Cell Count Acc. \\
		\addlinespace[0.3em]
		
		% --- 2022 ---
		\multicolumn{5}{l}{\textbf{2022}} \\
		\addlinespace[0.2em]
		
		\rowcolor{gray!15}
		\textcite{Banerjee2022Falcon} & 2022 & Malaria: Binary Cell Classif. (NIH, Thin N/S) & Falcon DCNN, TL & Custom DCNN+TL; 95.2\% Acc. \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{LiMa2022ResidualAttentionSVM} & 2022 & Malaria: Binary Cell Classif. (NIH, Thin N/S) & Residual Attn. CNN + SVM & Hybrid CNN-SVM; 99.7\% Acc. \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{AlonsoRamirez2022CNNRecurrent} & 2022 & Malaria: Binary Cell Classif. (NLM NIH, Thin N/S) & CNN + LSTM/BiLSTM & CNN-RNN hybrid; 99.89\% Acc. \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Jameela2022DeepLearning} & 2022 & Malaria: Cell Classif. (NIH, Thin Smear) & ResNet50, ResNet34, VGG-16, VGG-19 TL/FT & Best VGG-19 Acc. \\
		\addlinespace[0.3em]
		
		% --- 2023 ---
		\multicolumn{5}{l}{\textbf{2023}} \\
		\addlinespace[0.2em]
		
		\rowcolor{gray!15}
		\textcite{sukumarran_automated_2023} & 2023 & Malaria Det. (MP-IDB/ Giemsa Thin) & YOLOv4, Faster R-CNN, SSD300 & YOLOv4: Prec. 83\%, Rec. 95\%, F1 89\%, mAP 93.87\% \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Khan2023CNNBTSystems} & 2023 & Malaria: Binary Cell Classif. (NLM NIH, Thin N/S) & Inception-ResNet TL & 95\% Acc. \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Goni2023DELMMalaria} & 2023 & Malaria: Binary Cell Classif. (Kaggle NIH derived, Thin N/S) & Light CNN + DELM & CNN feat. + DELM; 99.66\% Acc. \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Silka2023MalariaDetection} & 2023 & Malaria: Cell Det. (BBBC041, Thin Giemsa) & Custom Semantic Seg. CNN & 99.68\% Acc. (Det. via Seg.); 97.1\% pixel Acc. \\
		\addlinespace[0.3em]
		
		% --- 2024 ---
		\multicolumn{5}{l}{\textbf{2024}} \\
		\addlinespace[0.2em]
		
		\rowcolor{gray!15}
		\textcite{HoyosHoyos2024YOLOv8Malaria} & 2024 & Malaria: Parasite/Leukocyte Det. (NIH, Thin N/S) & YOLOv8 & 95\% Acc. (Parasite) \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Ramos2024TransferLearning} & 2024 & Malaria: Plasmodium vivax ROI Classif. (BBBC+FIOCRUZ, Thin Giemsa) & DenseNet201, MobileNetV2, VGG19, etc. TL & DenseNet201: 99.41\% AUC \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Ohdar2024MultiSpecies} & 2024 & Malaria: Multi-Species Classif. (MP-IDB, Thin N/S) & Xception, EfficientNet, MobileNet, DenseNet121 Ensemble TL & 98.9\% Acc. (4 species) \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Boit2024MalariaDetection} & 2024 & Malaria: RBC Classif. (NIH, Thin Giemsa) & EDRI (Hybrid DL Model) & 97.68\% Acc. \\
		\addlinespace[0.3em]
		
		% --- 2025 ---
		\multicolumn{5}{l}{\textbf{2025}} \\
		\addlinespace[0.2em]
		
		\rowcolor{gray!15}
		\textcite{RamosBriceno2025MalariaSpecies} & 2025 & Malaria: Species Classif. (NLM, Thick Giemsa) & Custom CNN (7-ch. input, preprocessing) & 99.51\% Acc. (P. falciparum, P. vivax, uninfected WBC) \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{SoraCardenas2025MalariaThickSmear} & 2025 & Malaria: Parasite/Leukocyte Det. \& Classif. (Thick Romanowsky) & SVM (Quality), OTSU (Leukocyte), Custom CNN (Parasite) & Parasite Det. F1: 82.10\%; Stage Classif. F1: 83-88\% \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Mmileng2025ConvNeXtMalaria} & 2025 & Malaria: Parasite Classif. (NLM Thin Smear) & ConvNeXt Tiny (V1/V2 mod.) TL/DA & ConvNeXt V2 Tiny: 98.1\% Acc. \\
		\addlinespace[0.3em]
		\rowcolor{gray!15}
		\textcite{Nasra2025AutomatedMalaria} & 2025 & Malaria: Parasite Det. (Lacuna, Thick/Thin) & Optimized CNN Architecture & Approx. 98\% Acc.\\
	\end{longtable}
\end{center}

%\begin{center}
%	\parbox{0.9\linewidth}{
%		\scriptsize
%		\textbf{Note:} Gray background indicates malaria studies. Key datasets: NIH \parencite{Rajaraman2018NIHMalariaDataset}, Lacuna \parencite{LacunaMalariaDatasets} MP-IDB \cite{MP-IDB}.
%		\vspace{0.3em}
%		
%		\textbf{Abbreviations:} Acc.: Accuracy; Attn.: Attention; Classif.: Classification; CNN: Convolutional Neural Network; Det.: Detection; F1: F1-Score; FT: Fine-Tuning; mAP: Mean Average Precision; N/S: Not Specified; NLM: National Library of Medicine; Perf.: Performance; Prec.: Precision; RBC: Red Blood Cell; Rec.: Recall; ROI: Region of Interest; Seg.: Segmentation; TL: Transfer Learning; WBC: White Blood Cell; WSI: Whole Slide Imaging.
%	}
%\end{center}
%\vspace{\baselineskip}

\noindent
Beyond fine-tuning, several works explored ensemble strategies \parencite{Nayak2022EnsembleAI, Bibin2017MalariaDBN}, hybrid pipelines mixing CNN features with classifiers such as SVMs \parencite{LiMa2022ResidualAttentionSVM}, and object detection based on YOLO variants \parencite{Abdurahman2021ModifiedYOLO, HoyosHoyos2024YOLOv8Malaria}. Data augmentation is also widely used because annotated samples are limited. Although these methods mark progress, important limitations remain. Model outputs are often partial and do not cover all steps needed for clinical diagnosis, and performance tends to drop when exposed to variations in clinics, staining quality, or imaging devices. Reproducibility is also weakened by inconsistent reporting of smear types, dataset curation, or preprocessing choices.\\

\noindent
Transfer learning has been helpful for automated malaria classification, yet most studies focus on the binary separation between infected and uninfected cells. A complete diagnosis requires more information, including species, life stage, and parasitemia. Since standard TL approaches usually target a single prediction objective, there is space for techniques that better reflect the multi-step nature of real diagnosis.

\subsection{Multitask and Multitask Transfer Learning Relevance}
\label{ssec:mtl_mttl_malaria}

\noindent
Multitask learning (\ac{MTL}) and it's extension multitask transfer learning (MTTL) offer a way to learn several related outputs together. Tasks such as species identification, life-stage prediction, and parasitemia estimation share visual cues, so a shared backbone with transferred features can capture these relations more effectively than training independent models. Despite this fit, no systematic use of MTL or MTTL has been reported for malaria imaging. In other medical imaging areas, \ac{MTL} and \ac{MTTL} have shown clear value. \textcite{Yadav2024_MT_TL_ActiveMedSeg} used multitask transfer learning within an active learning setup for segmentation and reported better annotation efficiency. \textcite{Huang2022MTLABS3Net} introduced a multitask system combining organ segmentation with anatomical prior prediction and obtained strong semi-supervised performance. \textcite{Kim2023CrossTaskAttention} presented a cross-task attention mechanism to improve multi-task learning across diverse medical applications. \textcite{Chandani2023Investigating} and \textcite{Dheer2023Exploiting} studied MTTL approaches for segmentation, showing that shared representations improve accuracy. \textcite{Schoenpflug2023_MTL_CRC} applied multitask learning to colorectal cancer histology slides to segment tissues and detect tumors, while \textcite{Li2023_MTL_NSCLC} used MTL to classify histologic subtypes and tumor grade in non-small cell lung cancer CT scans. In robotic surgery, \textcite{Islam2021StMTLSpatioTemporal} developed a spatio-temporal multitask model predicting instrument paths and surgeon scanpaths. Applications outside medicine, including NLP and multi-view autonomous driving \parencite{Zhang2019_MultiviewTL_MTL, raffel2020exploring}, also report gains in efficiency and robustness.\\

\noindent
Together, these studies show that multitask and multitask transfer learning can capture shared structure across tasks, improve generalization, and avoid training separate models. This opportunity remains mostly unexplored in malaria diagnosis.

\subsection{Observed Gaps in the Literature}
\label{ssec:literature_gaps}

\noindent
Even with recent progress, key gaps still limit clinical use when malaria diagnosis is viewed as a full multi-step procedure. Main issues include:

\begin{itemize}
	\item \textbf{Incomplete outputs:} Most studies classify cells as either infected or uninfected, which does not provide species, life stage, or parasitemia needed for treatment decisions.
	\item \textbf{Lack of multitask approaches:} No work has applied MTL or MTTL to malaria imaging even though this mirrors the workflow of microscopic diagnosis.
	\item \textbf{Theoretical limitations:} Existing research does not offer a formal multitask transfer framework adapted to malaria or similar diagnostic processes.
	\item \textbf{Reporting inconsistencies:} Details like smear type, staining protocol, and preprocessing are often missing, which complicates reproducibility.
	\item \textbf{Dataset constraints:} Public datasets are small and homogeneous, reducing the ability of models to generalize across clinics or devices.\\
\end{itemize}

\noindent Prior work confirms the strengths of transfer learning but remains narrow in scope. The absence of multitask methods, combined with limited datasets and incomplete diagnostic outputs, highlights the need for more comprehensive approaches. This motivates the introduction of a multitask transfer learning framework for malaria diagnosis.