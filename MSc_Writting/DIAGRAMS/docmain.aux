\relax 
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Principle of the convolution operation.}}{1}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:convolution_simplified}{{1}{1}{}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Max pooling with 2x2 windows from input to output.}}{1}{}\protected@file@percent }
\newlabel{fig:max_pooling_simple}{{2}{1}{}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \textbf  {The LoRA Module.} This is the base of our architecture, the adapter modifies an input feature vector $\mathbf  {z}$ using a trainable low-rank path. This path projects $\mathbf  {z}$ through $W_A \in \mathbb  {R}^{C \times r}$ and $W_B \in \mathbb  {R}^{r \times C}$, before scaling the result and adding it back to the original input. The equation is: $\mathbf  {z}' = \mathbf  {z} + \tfrac  {\alpha }{r} \, W_B W_A \mathbf  {z}$. }}{2}{}\protected@file@percent }
\newlabel{fig:lora_adapter}{{3}{2}{}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \textbf  {LoRA Adaptation of a ResNet Block.} In our implementation, the entire pre-trained ResNet block (Stage), including its internal convolutions and skip connection, is treated as a single frozen feature extractor. Our lightweight, trainable LoRA adapter is then applied to the output of this block. This allows the adapter to learn task-specific residual correction on top of the sophisticated features generated by the original block. }}{2}{}\protected@file@percent }
\newlabel{fig:adapted_resnet_block_postfix}{{4}{2}{}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Adapted ResNet-50 Backbone.} Our frozen ResNet-50 provides feature maps (Stages 1–4), then the trainable LoRA adapters (orange) are applied after each stage to produce the adapted feature maps. Outputs are taken from multi-scale stages (C2–C4) and a final global 2048-D feature vector after GAP.}}{3}{}\protected@file@percent }
\newlabel{fig:compact_full_backbone}{{5}{3}{}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \textbf  {Architecture of our Custom Faster R-CNN Detection Head.} First we grab Multi-scale adapted features from backbone (C2–C4, orange) feed the FPN to produce pyramid features (P2–P4, cyan). A "pyramid bus" distributes these features to the RPN (for proposals) and RoIAlign. RoIAlign pools features into 7×7 maps with N channels (N×64×7×7), which are then processed by the detailed \textbf  {Lightweight Box Head} to produce class scores and bounding box offsets. }}{4}{}\protected@file@percent }
\newlabel{fig:detection_head_arch_final_v4}{{6}{4}{}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  \textbf  {Architecture of our Localization Heatmap Head.} High-resolution features from two parallel paths (C1 and C2) are projected and fused via concatenation. A spatial attention mechanism refines these features, which are then processed by a decoder path (blue). A final 1x1 convolution, ReLU, and Sigmoid activation produce the 64×64 single-channel localization heatmap (green). }}{5}{}\protected@file@percent }
\newlabel{fig:heatmap_head_arch_final_grouped}{{7}{5}{}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  \textbf  {Architecture of our Segmentation Head.} Our segmentation head is inspired by the UNet-style decoder which progressively upsamples low-resolution features while merging high-resolution features from the backbone (orange zone) via skip connections (dashed lines). The decoder path (blue zone) fuses these features at each stage, and a final 1x1 convolution outputs the segmentation mask logits. }}{6}{}\protected@file@percent }
\newlabel{fig:segmentation_head_arch_final_polished_v6}{{8}{6}{}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  \textbf  {Classification Head Architectures.} \textbf  {(Left)} Simple MLP for image-level tasks (Cell classif from 64x64 patches and Severity estimation), processing the `B x 2048` global feature vector from our adapted Resnet-50 backbone. \textbf  {(Right)} RoI Head for object-level tasks (ROI-Based classification of bboxes), pooling features from the FPN for a variable number of `RoIs` before processing them with a similar MLP. }}{7}{}\protected@file@percent }
\newlabel{fig:detailed_classification_heads_full_dims}{{9}{7}{}{figure.9}{}}
\gdef \@abspage@last{7}
